{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#bean-jam-bot","title":"\ud83e\uded8 Bean Jam Bot","text":""},{"location":"#features","title":"\u2728 Features","text":""},{"location":"#ai-powered-conversations","title":"\ud83e\udd16 AI-Powered Conversations","text":"<ul> <li>Google Gemini Pro integration for intelligent, context-aware restaurant and dating spot recommendations</li> <li>Natural language understanding for planning dates, restaurant hopping sessions, or getting quick recommendations</li> <li>Conversation history tracking for coherent, multi-turn dialogues</li> <li>Smart itinerary planning assistance</li> </ul>"},{"location":"#restaurant-dating-planning-intelligence","title":"\ud83d\uddfa\ufe0f Restaurant &amp; Dating Planning Intelligence","text":"<ul> <li>Curates date ideas based on vibe (cozy, adventurous, budget, premium) and time of day</li> <li>Builds restaurant-hopping routes with proximity awareness and cuisine diversity</li> <li>Adjusts plans using live weather context (e.g., indoor/outdoor, sunset views)</li> <li>Bilingual recommendations (EN/JP) with culturally relevant suggestions</li> </ul>"},{"location":"#advanced-voice-input","title":"\ud83c\udfa4 Advanced Voice Input","text":"<ul> <li>Google Cloud Speech-to-Text integration with 48kHz WEBM_OPUS encoding</li> <li>Real-time voice transcription in English and Japanese</li> <li>Visual feedback during recording with reactive blob animation</li> <li>Automatic audio level detection and visualization with 8-band equalizer</li> <li>Helpful toast notifications guiding users through the recording process</li> </ul>"},{"location":"#bilingual-support","title":"\ufffd Bilingual Support","text":"<ul> <li>Seamless language switching between English (EN) and Japanese (JP)</li> <li>Language-specific AI prompts and responses</li> <li>Fully localized UI strings and error messages</li> <li>Cultural context awareness in recommendations</li> </ul>"},{"location":"#dynamic-blob-animation","title":"\ud83c\udf0a Dynamic Blob Animation","text":"<ul> <li>Organic, living blob that reacts to user interactions</li> <li>Voice-reactive animation with real-time audio level visualization</li> <li>Smooth state transitions between idle and active modes</li> <li>Different visual modes:</li> <li>Normal Mode: Teal/blue gradient for standard conversations</li> <li>Create Mode: Purple/pink gradient when AI is generating responses</li> <li>Gradual rotation and morphing effects</li> <li>Adjustable blur and saturation based on activity</li> <li>Hardware-accelerated animations for smooth 60fps performance</li> </ul>"},{"location":"#weather-integration","title":"\ud83c\udf24\ufe0f Weather Integration","text":"<ul> <li>Context-aware weather detection - Gemini AI decides when to show weather cards</li> <li>Real-time weather data from WeatherAPI.com</li> <li>Multi-day forecast support (current + up to 7 days)</li> <li>Location-based recommendations using browser geolocation</li> <li>IP-based fallback using OpenStreetMap for reverse geocoding</li> <li>Beautiful weather card UI with temperature, conditions, and forecasts</li> </ul>"},{"location":"#modern-uiux","title":"\ud83c\udfa8 Modern UI/UX","text":"<ul> <li>Brutalist design aesthetic with bold borders and sharp edges</li> <li>Glassmorphism chat panel with adjustable transparency</li> <li>Smooth animations and transitions (1s opacity fades, 0.8s transforms)</li> <li>Custom scrollbar styling for chat history</li> <li>Responsive layout optimized for all screen sizes</li> <li>Brutalist shadow effects for depth</li> <li>Audio level equalizer overlay during voice input</li> </ul>"},{"location":"#architecture-production-full-serverless","title":"\ud83c\udfd7\ufe0f Architecture (Production - Full Serverless)","text":"<pre><code>flowchart TD\n    Browser[\"&lt;b&gt;Browser Client&lt;/b&gt;&lt;br/&gt;React + Vite UI \u2022 Voice (MediaRecorder) \u2022 Blob Animation \u2022 EN/JP\"]\n\n    Browser --&gt;|Audio| Gateway\n    Browser --&gt;|Chat| Gateway\n    Browser --&gt;|Weather| Gateway\n\n    Gateway[\"&lt;b&gt;AWS API Gateway (HTTP API)&lt;/b&gt;&lt;br/&gt;CORS @ Edge \u2022 Payload Format v2.0&lt;br/&gt;&lt;br/&gt;/transcribe    /gemini    /weather\"]\n\n    Gateway --&gt; L1\n    Gateway --&gt; L2\n    Gateway --&gt; L3\n\n    L1[\"&lt;b&gt;Lambda: Transcribe&lt;/b&gt;&lt;br/&gt;Node 20\"]\n    L2[\"&lt;b&gt;Lambda: Gemini&lt;/b&gt;&lt;br/&gt;Node 20\"]\n    L3[\"&lt;b&gt;Lambda: Weather&lt;/b&gt;&lt;br/&gt;Node 20\"]\n\n    L1 --&gt; API1\n    L2 --&gt; API2\n    L3 --&gt; API3\n\n    API1[\"&lt;b&gt;Google Cloud&lt;/b&gt;&lt;br/&gt;Speech-to-Text (v2)\"]\n    API2[\"&lt;b&gt;Google Gemini&lt;/b&gt;&lt;br/&gt;Generative Language API\"]\n    API3[\"&lt;b&gt;WeatherAPI&lt;/b&gt;&lt;br/&gt;Forecast API\"]\n\n    style Browser fill:#5B9BD5,stroke:#2E5C8A,stroke-width:4px,color:#fff,font-weight:bold\n    style Gateway fill:#70AD47,stroke:#507E32,stroke-width:4px,color:#fff,font-weight:bold\n    style L1 fill:#FFC000,stroke:#C09000,stroke-width:3px,color:#000,font-weight:bold\n    style L2 fill:#FFC000,stroke:#C09000,stroke-width:3px,color:#000,font-weight:bold\n    style L3 fill:#FFC000,stroke:#C09000,stroke-width:3px,color:#000,font-weight:bold\n    style API1 fill:#9966FF,stroke:#7744CC,stroke-width:3px,color:#fff,font-weight:bold\n    style API2 fill:#9966FF,stroke:#7744CC,stroke-width:3px,color:#fff,font-weight:bold\n    style API3 fill:#9966FF,stroke:#7744CC,stroke-width:3px,color:#fff,font-weight:bold</code></pre> <p>Architecture Benefits: - \u2705 API keys secured server-side (never exposed to client) - \u2705 Single CORS point at API Gateway (no duplicate headers) - \u2705 Auto-scaling with Lambda cold-start &lt; 500ms - \u2705 Cost-efficient (pay-per-request, ~$0.20/million requests) - \u2705 Unified monitoring via CloudWatch Logs &amp; Metrics</p> <p>High-Level Overview:</p> <pre><code>flowchart TB\n    Client[Browser Client&lt;br/&gt;React + Vite UI&lt;br/&gt;Voice \u2022 Blob \u2022 EN/JP]\n\n    Client --&gt;|Audio| Gateway\n    Client --&gt;|Chat| Gateway\n    Client --&gt;|Weather| Gateway\n\n    Gateway[AWS API Gateway&lt;br/&gt;HTTP API&lt;br/&gt;CORS @ Edge]\n\n    Gateway --&gt;|/transcribe| L1[Lambda&lt;br/&gt;Transcribe&lt;br/&gt;Node 20]\n    Gateway --&gt;|/gemini| L2[Lambda&lt;br/&gt;Gemini&lt;br/&gt;Node 20]\n    Gateway --&gt;|/weather| L3[Lambda&lt;br/&gt;Weather&lt;br/&gt;Node 20]\n\n    L1 --&gt; STT[Google Cloud&lt;br/&gt;Speech-to-Text]\n    L2 --&gt; GEM[Google Gemini&lt;br/&gt;API]\n    L3 --&gt; WEA[WeatherAPI&lt;br/&gt;Forecast]\n\n    style Client fill:#3b82f6,stroke:#1e40af,color:#fff\n    style Gateway fill:#10b981,stroke:#059669,color:#fff\n    style L1 fill:#f59e0b,stroke:#d97706,color:#fff\n    style L2 fill:#f59e0b,stroke:#d97706,color:#fff\n    style L3 fill:#f59e0b,stroke:#d97706,color:#fff\n    style STT fill:#8b5cf6,stroke:#7c3aed,color:#fff\n    style GEM fill:#8b5cf6,stroke:#7c3aed,color:#fff\n    style WEA fill:#8b5cf6,stroke:#7c3aed,color:#fff</code></pre> <p>Detailed Architecture:</p> <pre><code>flowchart TD\n    subgraph Client[Browser Client - React + Vite]\n      UI[Chat UI&lt;br/&gt;EN/JP Toggle]\n      MIC[Voice Recorder&lt;br/&gt;MediaRecorder]\n      BLOB[Blob Animation&lt;br/&gt;Audio Reactive]\n    end\n\n    subgraph AWS[AWS Cloud - eu-north-1]\n      APIGW[API Gateway HTTP API&lt;br/&gt;CORS @ Edge Layer]\n\n      subgraph Lambdas[Lambda Functions - Node.js 20]\n        L1[BeanJamTranscribe&lt;br/&gt;512MB \u2022 30s timeout]\n        L2[BeanJamGemini&lt;br/&gt;256MB \u2022 15s timeout]\n        L3[BeanJamWeather&lt;br/&gt;256MB \u2022 15s timeout]\n      end\n    end\n\n    subgraph External[External APIs]\n      STT[Google Cloud&lt;br/&gt;Speech-to-Text v2]\n      GEMINI[Google Gemini&lt;br/&gt;2.5 Flash API]\n      WEATHER[WeatherAPI.com&lt;br/&gt;Forecast JSON]\n    end\n\n    MIC --&gt;|webm/opus audio| UI\n    UI --&gt;|POST /transcribe| APIGW\n    UI --&gt;|POST /gemini| APIGW\n    UI --&gt;|POST /weather| APIGW\n\n    APIGW --&gt;|v2.0 payload| L1\n    APIGW --&gt;|v2.0 payload| L2\n    APIGW --&gt;|v2.0 payload| L3\n\n    L1 --&gt;|recognize| STT\n    L2 --&gt;|generateContent| GEMINI\n    L3 --&gt;|forecast.json| WEATHER\n\n    STT -.transcript.-&gt; L1\n    GEMINI -.response.-&gt; L2\n    WEATHER -.forecast.-&gt; L3\n\n    L1 -.JSON.-&gt; APIGW\n    L2 -.JSON.-&gt; APIGW\n    L3 -.JSON.-&gt; APIGW\n\n    APIGW -.response.-&gt; UI\n    UI --&gt; BLOB</code></pre>"},{"location":"#technology-stack","title":"\ud83d\ude80 Technology Stack","text":"Category Technology Frontend React 18, TypeScript, Vite UI Framework Tailwind CSS, Shadcn/ui, Radix UI State Management React Context API, TanStack Query Routing React Router AI Services Google Gemini 2.5 Flash, Google Cloud Speech-to-Text APIs WeatherAPI.com, OpenStreetMap Nominatim Backend (Dev) Node.js, Express Backend (Prod) AWS API Gateway (HTTP) + AWS Lambda (Node.js 20) Animation CSS Keyframes, RequestAnimationFrame, Web Audio API Deployment AWS Lambda/API Gateway, Docker (optional)"},{"location":"#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>bean-jam-bot/\n\u251c\u2500\u2500 aws-lambda/\n\u2502   \u251c\u2500\u2500 transcribe/              # AWS Lambda: Audio \u2192 Text\n\u2502   \u2502   \u251c\u2500\u2500 index.js             # Google Cloud STT handler\n\u2502   \u2502   \u251c\u2500\u2500 package.json\n\u2502   \u2502   \u2514\u2500\u2500 function.zip         # Deployment artifact\n\u2502   \u251c\u2500\u2500 gemini/                  # AWS Lambda: AI Chat Proxy\n\u2502   \u2502   \u251c\u2500\u2500 index.js             # Gemini API proxy handler\n\u2502   \u2502   \u2514\u2500\u2500 function.zip\n\u2502   \u2514\u2500\u2500 weather/                 # AWS Lambda: Weather Proxy\n\u2502       \u251c\u2500\u2500 index.js             # WeatherAPI.com proxy handler\n\u2502       \u2514\u2500\u2500 function.zip\n\u251c\u2500\u2500 server/                      # Local development server (optional)\n\u2502   \u2514\u2500\u2500 index.js\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 components/\n\u2502   \u2502   \u251c\u2500\u2500 ui/                 # Shadcn/ui components\n\u2502   \u2502   \u251c\u2500\u2500 BlobAnimation.tsx   # \ud83c\udf0a Voice-reactive blob\n\u2502   \u2502   \u251c\u2500\u2500 ChatMessage.tsx     # Chat bubble component\n\u2502   \u2502   \u251c\u2500\u2500 WeatherCard.tsx     # Weather display card\n\u2502   \u2502   \u2514\u2500\u2500 LanguageToggle.tsx  # EN/JP switcher\n\u2502   \u251c\u2500\u2500 contexts/\n\u2502   \u2502   \u2514\u2500\u2500 useLanguage.tsx     # Language context\n\u2502   \u251c\u2500\u2500 hooks/\n\u2502   \u2502   \u251c\u2500\u2500 use-audio-recorder.ts  # \ud83c\udfa4 Voice recording\n\u2502   \u2502   \u251c\u2500\u2500 use-audio-level.ts     # Audio visualization\n\u2502   \u2502   \u2514\u2500\u2500 use-toast.ts\n\u2502   \u251c\u2500\u2500 lib/\n\u2502   \u2502   \u251c\u2500\u2500 gemini.ts           # Gemini API client\n\u2502   \u2502   \u251c\u2500\u2500 location.ts         # Geolocation service\n\u2502   \u2502   \u251c\u2500\u2500 weather.ts          # Weather API client\n\u2502   \u2502   \u2514\u2500\u2500 utils.ts\n\u2502   \u251c\u2500\u2500 pages/\n\u2502   \u2502   \u2514\u2500\u2500 Index.tsx           # Main chat interface\n\u2502   \u251c\u2500\u2500 App.tsx\n\u2502   \u2514\u2500\u2500 main.tsx\n\u251c\u2500\u2500 .env                         # Environment variables\n\u2514\u2500\u2500 Dockerfile                   # Docker containerization (optional)\n</code></pre>"},{"location":"#installation-setup","title":"\ud83d\udee0\ufe0f Installation &amp; Setup","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Node.js v18+</li> <li>Google Cloud Platform account (for Speech-to-Text)</li> <li>Google Gemini API key</li> <li>WeatherAPI.com API key</li> </ul>"},{"location":"#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/Defalt-here/Bean-Jam-Bot.git\ncd Bean-Jam-Bot\n</code></pre>"},{"location":"#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code>npm install\n</code></pre>"},{"location":"#3-configure-environment-variables","title":"3. Configure Environment Variables","text":"<p>Create a <code>.env</code> file:</p> <pre><code># Google Cloud Speech-to-Text (absolute path to service account JSON)\nGOOGLE_APPLICATION_CREDENTIALS=C:\\path\\to\\your\\service-account.json\n\n# Google Gemini API Key\nVITE_GEMINI_API_KEY=AIzaSy...\n\n# WeatherAPI.com API Key\nVITE_WEATHER_API_KEY=your_weatherapi_key\n\n# AWS Lambda Endpoints (API Gateway HTTP API)\nVITE_TRANSCRIBE_API_URL=https://&lt;api-id&gt;.execute-api.eu-north-1.amazonaws.com/transcribe\nVITE_GEMINI_PROXY_URL=https://&lt;api-id&gt;.execute-api.eu-north-1.amazonaws.com/gemini\nVITE_WEATHER_PROXY_URL=https://&lt;api-id&gt;.execute-api.eu-north-1.amazonaws.com/weather\n</code></pre>"},{"location":"#4-run-development-servers","title":"4. Run Development Servers","text":"<p>Terminal 1 - Backend Server: </p><pre><code>npm run start:server\n# or use the PowerShell script:\n.\\start-server.ps1\n</code></pre><p></p> <p>Terminal 2 - Frontend: </p><pre><code>npm run dev\n</code></pre><p></p> <p>Open <code>http://localhost:8080</code> in your browser! \ud83c\udf89</p>"},{"location":"#usage","title":"\ud83c\udfae Usage","text":""},{"location":"#text-chat","title":"Text Chat","text":"<ol> <li>Type your message in the input field</li> <li>Press Enter or click SEND</li> <li>AI responds with restaurant/dating recommendations</li> </ol>"},{"location":"#voice-input","title":"Voice Input","text":"<ol> <li>Click the Mic button</li> <li>Grant microphone permissions</li> <li>Speak your question (in English or Japanese)</li> <li>Click Stop to transcribe</li> <li>Message automatically sends to AI</li> </ol>"},{"location":"#language-switching","title":"Language Switching","text":"<ul> <li>Click the EN / \ud83c\uddef\ud83c\uddf5 toggle at the top</li> <li>Switches both UI and AI response language</li> <li>All prompts and responses adapt automatically</li> </ul>"},{"location":"#weather-queries","title":"Weather Queries","text":"<ul> <li>Ask about weather: \"What's the weather like?\"</li> <li>AI automatically shows weather card with forecast</li> <li>Location-based using your IP or GPS</li> </ul>"},{"location":"#deployment","title":"\ud83c\udf0d Deployment","text":""},{"location":"#aws-production-full-serverless","title":"AWS (Production - Full Serverless)","text":""},{"location":"#1-package-lambda-functions","title":"1\ufe0f\u20e3 Package Lambda Functions","text":"<p>Transcribe Lambda (has dependencies): </p><pre><code>cd aws-lambda/transcribe\nnpm install --omit=dev\nCompress-Archive -Path @(\"index.js\",\"package.json\",\"node_modules\") -DestinationPath function.zip -Force\n</code></pre><p></p> <p>Gemini &amp; Weather Lambdas (no dependencies, already packaged): </p><pre><code># function.zip already exists in:\n# aws-lambda/gemini/function.zip\n# aws-lambda/weather/function.zip\n</code></pre><p></p>"},{"location":"#2-create-lambda-functions","title":"2\ufe0f\u20e3 Create Lambda Functions","text":"Function Name Runtime Memory Timeout Env Vars BeanJamTranscribe Node.js 20.x 512 MB 30s <code>GOOGLE_SERVICE_ACCOUNT_KEY</code> = your GCP service account JSON BeanJamGemini Node.js 20.x 256 MB 15s <code>GEMINI_API_KEY</code> = your Gemini API key BeanJamWeather Node.js 20.x 256 MB 15s <code>WEATHER_API_KEY</code> = your WeatherAPI key <p>Steps for each: 1. Create Function \u2192 Author from scratch 2. Runtime: Node.js 20.x 3. Execution role: Use existing \u2192 <code>BeanJamTranscribeLambdaRole</code> (or create new with basic Lambda permissions) 4. Upload <code>function.zip</code> from Code tab 5. Configuration \u2192 Environment variables \u2192 Add key/value from table above 6. Configuration \u2192 General \u2192 Set Memory &amp; Timeout</p>"},{"location":"#3-create-api-gateway-http-api","title":"3\ufe0f\u20e3 Create API Gateway (HTTP API)","text":"<ol> <li>Create API</li> <li>Type: HTTP API</li> <li>Name: <code>BeanJamAPI</code></li> <li>Stage: <code>$default</code> (Auto-deploy: ON)</li> <li> <p>Region: <code>eu-north-1</code> (or your preferred region)</p> </li> <li> <p>Configure Routes</p> </li> <li>Click Routes \u2192 Create for each:</li> </ol> Method Path Integration Payload Format POST <code>/transcribe</code> Lambda: BeanJamTranscribe 2.0 POST <code>/gemini</code> Lambda: BeanJamGemini 2.0 POST <code>/weather</code> Lambda: BeanJamWeather 2.0 <ol> <li>Configure CORS (API-level)</li> <li>Click CORS in left menu</li> <li>Allow origins: <code>http://localhost:8080</code><ul> <li>Add production domains later (comma-separated)</li> </ul> </li> <li>Allow methods: <code>POST,OPTIONS</code></li> <li>Allow headers: <code>content-type</code></li> <li>Max age: <code>3600</code></li> <li> <p>\u2705 Critical: Do NOT set CORS headers in Lambda code (already handled correctly)</p> </li> <li> <p>Deploy</p> </li> <li>Toggle Auto-deploy OFF \u2192 ON to force refresh</li> <li> <p>Note your Invoke URL: <code>https://&lt;api-id&gt;.execute-api.eu-north-1.amazonaws.com</code></p> </li> <li> <p>Permissions</p> </li> <li>API Gateway will auto-add <code>lambda:InvokeFunction</code> permissions to each Lambda</li> <li>Verify in Lambda \u2192 Configuration \u2192 Permissions \u2192 Resource-based policy statements</li> </ol>"},{"location":"#4-update-frontend-configuration","title":"4\ufe0f\u20e3 Update Frontend Configuration","text":"<p>Edit <code>.env</code> with your API Gateway invoke URL:</p> <pre><code>VITE_TRANSCRIBE_API_URL=https://k3i65afofi.execute-api.eu-north-1.amazonaws.com/transcribe\nVITE_GEMINI_PROXY_URL=https://k3i65afofi.execute-api.eu-north-1.amazonaws.com/gemini\nVITE_WEATHER_PROXY_URL=https://k3i65afofi.execute-api.eu-north-1.amazonaws.com/weather\n</code></pre> <p>Restart Vite dev server: </p><pre><code>npm run dev\n</code></pre><p></p> <p>Hard-refresh browser: <code>Ctrl+Shift+R</code> (Windows) or <code>Cmd+Shift+R</code> (Mac)</p>"},{"location":"#6-optional-health-check-monitoring","title":"6\ufe0f\u20e3 Optional: Health Check &amp; Monitoring","text":"<p>Health Check Route: - API Gateway \u2192 Routes \u2192 Create \u2192 <code>GET /health</code> - Integration: Mock response - Response: <code>200</code> with body <code>{ \"ok\": true }</code> - Use for uptime monitoring (Pingdom, UptimeRobot, etc.)</p> <p>CloudWatch Monitoring: - Lambda \u2192 Monitor \u2192 View CloudWatch logs - API Gateway \u2192 Monitor \u2192 CloudWatch metrics - Set alarms for 5XX errors, cold start duration, invocation counts</p>"},{"location":"#blob-animation-technical-details","title":"\ud83c\udfa8 Blob Animation Technical Details","text":"<p>The blob animation is a sophisticated visual element that enhances user engagement:</p> <ul> <li>Interpolated Loading States: Smooth transitions using lerp (linear interpolation) with 0.05 speed</li> <li>Audio Reactivity: Real-time FFT analysis creates 8-band equalizer visualization</li> <li>Dynamic Properties:</li> <li>Blur: 8-26px based on state</li> <li>Saturation: 120-200% during activity</li> <li>Rotation: 60s slow (idle) to 8s fast (loading)</li> <li>Scale: Pulsing 1.0-1.08x</li> <li>Movement: Sine/cosine wave patterns</li> <li>Color Modes:</li> <li>Normal: <code>#3B82F6</code> \u2192 <code>#10B981</code> \u2192 <code>#22D3EE</code> (blue-teal)</li> <li>Create: <code>#6366F1</code> \u2192 <code>#A855F7</code> \u2192 <code>#EC4899</code> (purple-pink)</li> <li>Performance: Hardware-accelerated with <code>will-change</code> transforms</li> </ul>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<p>This project is open source and available under the MIT License.</p>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":"<ul> <li>Google Gemini AI for conversational intelligence</li> <li>Google Cloud Speech-to-Text for voice recognition</li> <li>WeatherAPI.com for weather data</li> <li>Shadcn/ui for beautiful components</li> <li>The React and Vite communities</li> </ul>   **Made with \ud83d\udc9c by [Defalt-here](https://github.com/Defalt-here)**  *Bean Jam Bot - Where AI meets beautiful design* \u2728"},{"location":"#troubleshooting-aws","title":"\ud83e\uddf0 Troubleshooting (AWS)","text":"<ul> <li>405 Method Not Allowed</li> <li>Ensure route <code>POST /transcribe</code> is attached to your Lambda integration</li> <li>Toggle <code>$default</code> stage Auto-deploy OFF \u2192 ON to redeploy</li> <li> <p>Verify you\u2019re calling <code>/transcribe</code> path (not base URL)</p> </li> <li> <p>CORS blocked (No/duplicate Access-Control-Allow-Origin)</p> </li> <li>Let API Gateway own CORS; remove <code>Access-Control-*</code> headers from Lambda</li> <li> <p>CORS in API: origins = your exact origin(s), methods = POST, headers = content-type</p> </li> <li> <p>Function URL vs API Gateway</p> </li> <li>Prefer API Gateway for production (auth, observability, routes)</li> <li>If using Function URL temporarily, set CORS there and avoid duplicate headers in Lambda</li> </ul>"},{"location":"architecture/backend/","title":"Backend","text":""},{"location":"architecture/backend/#bean-jam-bot-technical-documentation","title":"Bean Jam Bot - Technical Documentation","text":""},{"location":"architecture/backend/#1-system-overview","title":"1. System Overview","text":"<p>The Bean Jam Bot backend is a modern, serverless-first system designed as a Backend-for-Frontend (BFF) and secure proxy layer. It orchestrates communication between the client-side application and various third-party AI and data services.</p>"},{"location":"architecture/backend/#11-architecture","title":"1.1. Architecture","text":"<p>The architecture is built on Node.js and embraces a microservices-oriented approach, with core functionalities encapsulated into distinct, independently deployable services. The system is designed for high scalability and flexibility, supporting multiple deployment targets:</p> <ul> <li>Serverless Functions: The primary deployment strategy, with support for Vercel, Netlify, and AWS Lambda. This approach offers pay-per-use scalability and minimal operational overhead.</li> <li>Containerization: A Dockerized Node.js/Express application is available for traditional container orchestration platforms like Google Cloud Run, AWS ECS, or Kubernetes.</li> <li>Local Development: A simple Express server mirrors the production API, enabling seamless local development and testing.</li> </ul>"},{"location":"architecture/backend/#12-core-responsibilities","title":"1.2. Core Responsibilities","text":"<p>The backend is stateless and serves three primary functions:</p> <ol> <li>Secure Credential Management: It securely stores and utilizes API keys and service account credentials for third-party services, preventing their exposure on the client side.</li> <li>API Proxying: It acts as a proxy to external APIs (Google Gemini, WeatherAPI.com), simplifying client-side logic and managing CORS.</li> <li>Business Logic &amp; Data Processing: It handles logic that is unsuitable for the client, such as audio data formatting for transcription and constructing complex prompts for the Gemini AI.</li> </ol>"},{"location":"architecture/backend/#2-project-structure","title":"2. Project Structure","text":"<p>The backend code is organized by deployment target, with shared logic patterns across each implementation.</p> <pre><code>.\n\u251c\u2500\u2500 api/                    # Vercel Serverless Functions\n\u2502   \u2514\u2500\u2500 transcribe.js       # Transcription endpoint for Vercel\n\u251c\u2500\u2500 aws-lambda/             # AWS Lambda Functions (most complete implementation)\n\u2502   \u251c\u2500\u2500 gemini/             # Gemini AI Proxy Service\n\u2502   \u2502   \u2514\u2500\u2500 index.js\n\u2502   \u251c\u2500\u2500 transcribe/         # Transcription Service\n\u2502   \u2502   \u251c\u2500\u2500 index.js\n\u2502   \u2502   \u2514\u2500\u2500 package.json\n\u2502   \u251c\u2500\u2500 weather/            # Weather API Proxy Service\n\u2502   \u2502   \u2514\u2500\u2500 index.js\n\u2502   \u2514\u2500\u2500 DEPLOY.md           # AWS deployment guide\n\u251c\u2500\u2500 netlify/                # Netlify Functions\n\u2502   \u2514\u2500\u2500 functions/\n\u2502       \u2514\u2500\u2500 transcribe.js   # Transcription endpoint for Netlify\n\u251c\u2500\u2500 server/                 # Local/Docker Express Server\n\u2502   \u2514\u2500\u2500 index.js\n\u2514\u2500\u2500 Dockerfile              # Docker configuration for the Express server\n</code></pre> <ul> <li><code>aws-lambda/</code>: Contains the most comprehensive set of backend services, each as a separate Lambda function. This represents the full microservices architecture.</li> <li><code>api/</code> &amp; <code>netlify/</code>: Provide simplified, single-function implementations for easy deployment on Vercel and Netlify platforms, respectively.</li> <li><code>server/</code>: A monolithic Express server for local development that mimics the <code>transcribe</code> API.</li> <li><code>Dockerfile</code>: Packages the <code>server/</code> application for container-based deployments.</li> </ul>"},{"location":"architecture/backend/#3-core-modules-and-services","title":"3. Core Modules and Services","text":"<p>The backend consists of three main services, each with a dedicated responsibility.</p>"},{"location":"architecture/backend/#31-transcription-service","title":"3.1. Transcription Service","text":"<p>This service processes audio data from the client and transcribes it into text using Google Cloud Speech-to-Text.</p> <ul> <li>Implementations:<ul> <li><code>aws-lambda/transcribe/index.js</code></li> <li><code>api/transcribe.js</code> (Vercel)</li> <li><code>netlify/functions/transcribe.js</code></li> <li><code>server/index.js</code> (Express)</li> </ul> </li> <li>Key Logic:<ol> <li>Receives a POST request with base64-encoded audio, its MIME type, and a language code.</li> <li>Initializes the <code>@google-cloud/speech</code> client using credentials stored securely in environment variables.</li> <li>A <code>mimeToEncoding</code> helper function intelligently maps browser-specific audio MIME types (e.g., <code>audio/webm;codecs=opus</code>, <code>audio/wav</code>) to the corresponding encoding format required by the Google Cloud API (e.g., <code>WEBM_OPUS</code>, <code>LINEAR16</code>).</li> <li>It correctly configures the <code>sampleRateHertz</code> for Opus-based codecs, which is a critical requirement for accurate transcription.</li> <li>The audio content is sent to the Google Cloud <code>recognize</code> endpoint.</li> <li>The response is parsed to extract and concatenate the final transcript, which is returned to the client.</li> </ol> </li> </ul>"},{"location":"architecture/backend/#32-gemini-ai-proxy-service","title":"3.2. Gemini AI Proxy Service","text":"<p>This service provides a secure interface to the Google Gemini API, constructing context-rich prompts to generate intelligent chat responses.</p> <ul> <li>Implementation: <code>aws-lambda/gemini/index.js</code></li> <li>Key Logic:<ol> <li>Receives the user's message, conversation history, language, and optional context (location, weather).</li> <li>Authenticates with the Gemini API using a server-side <code>GEMINI_API_KEY</code>.</li> <li>Dynamically builds a detailed system instruction that defines the AI's persona, language, and includes the user's current location and weather data.</li> <li>UI Control Logic: The system instruction includes a special directive for the AI to prepend its response with the marker <code>[SHOW_WEATHER_CARD]</code> if the user's query is weather-related. This allows the AI to control UI components on the frontend.</li> <li>The full conversation history is formatted and sent along with the new message to maintain context.</li> <li>The response from Gemini is parsed, the <code>[SHOW_WEATHER_CARD]</code> marker is removed from the text, and the final payload <code>{ response: string, showWeatherCard: boolean }</code> is returned to the client.</li> </ol> </li> </ul>"},{"location":"architecture/backend/#33-weather-api-proxy-service","title":"3.3. Weather API Proxy Service","text":"<p>This service securely fetches weather data from WeatherAPI.com based on the user's location.</p> <ul> <li>Implementation: <code>aws-lambda/weather/index.js</code></li> <li>Key Logic:<ol> <li>Accepts a location query (<code>q</code>) and number of forecast days (<code>days</code>) via GET or POST methods.</li> <li>Uses a server-side <code>WEATHER_API_KEY</code> to authenticate with WeatherAPI.com.</li> <li>Forwards the request to the <code>forecast.json</code> endpoint of the WeatherAPI.</li> <li>Returns the unmodified JSON response from the WeatherAPI directly to the client.</li> </ol> </li> </ul>"},{"location":"architecture/backend/#4-api-documentation","title":"4. API Documentation","text":""},{"location":"architecture/backend/#transcription-endpoint","title":"Transcription Endpoint","text":"<ul> <li>Endpoint: <code>POST /api/transcribe</code> (Vercel/Netlify/Local) or <code>POST /transcribe</code> (AWS API Gateway)</li> <li>Description: Transcribes a base64-encoded audio clip into text.</li> <li>Request Body: <code>application/json</code></li> </ul> Field Type Required Description <code>audioBase64</code> <code>string</code> Yes The raw base64-encoded audio data. <code>mimeType</code> <code>string</code> Yes The MIME type of the audio (e.g., <code>audio/webm;codecs=opus</code>). <code>languageCode</code> <code>string</code> No BCP-47 language code (e.g., <code>en-US</code>, <code>ja-JP</code>). Defaults to <code>en-US</code>. <ul> <li>Success Response (200 OK):</li> </ul> <pre><code>{\n  \"transcript\": \"hello world\",\n  \"raw\": { ... }\n}\n</code></pre> <ul> <li>Error Response (4xx/5xx):</li> </ul> <pre><code>{\n  \"error\": \"A detailed error message.\"\n}\n</code></pre> <ul> <li>Example <code>curl</code> Request:</li> </ul> <pre><code>curl -X POST https://&lt;your-api-url&gt;/transcribe \\\n-H \"Content-Type: application/json\" \\\n-d '{\n  \"audioBase64\": \"UklGRiQAAABXQVZFZm10IBAAAAABAAEARKwAAIhYAQACAB...\",\n  \"mimeType\": \"audio/wav\",\n  \"languageCode\": \"en-US\"\n}'\n</code></pre>"},{"location":"architecture/backend/#gemini-ai-endpoint","title":"Gemini AI Endpoint","text":"<ul> <li>Endpoint: <code>POST /gemini</code> (AWS API Gateway)</li> <li>Description: Generates a chat response from the Gemini AI model.</li> <li>Request Body: <code>application/json</code></li> </ul> Field Type Required Description <code>message</code> <code>string</code> Yes The user's current text message. <code>language</code> <code>string</code> No <code>en</code> or <code>jp</code>. Defaults to <code>en</code>. <code>conversationHistory</code> <code>Array&lt;object&gt;</code> No Array of <code>{ content: string, isUser: boolean }</code> objects. <code>userLocation</code> <code>string</code> No Formatted location string for AI context. <code>weatherData</code> <code>string</code> No Formatted weather summary for AI context. <ul> <li>Success Response (200 OK):</li> </ul> <pre><code>{\n  \"response\": \"Here are some great spots for a date...\",\n  \"showWeatherCard\": false\n}\n</code></pre>"},{"location":"architecture/backend/#weather-endpoint","title":"Weather Endpoint","text":"<ul> <li>Endpoint: <code>GET|POST /weather</code> (AWS API Gateway)</li> <li>Description: Fetches weather forecast data.</li> <li>GET Request Query Parameters:</li> </ul> Field Type Required Description <code>q</code> <code>string</code> Yes Location query (e.g., \"Paris\" or \"48.85,2.35\"). <code>days</code> <code>string</code> No Number of forecast days. Defaults to <code>1</code>. <ul> <li>POST Request Body: <code>application/json</code></li> </ul> Field Type Required Description <code>q</code> <code>string</code> Yes Location query. <code>days</code> <code>number</code> No Number of forecast days. Defaults to <code>1</code>. <ul> <li>Success Response (200 OK):<ul> <li>Returns the raw JSON response directly from the WeatherAPI.com service.</li> </ul> </li> </ul>"},{"location":"architecture/backend/#5-database-layer","title":"5. Database Layer","text":"<p>The backend is stateless. There is no database, as all conversation history and state are managed on the client side and passed with each API request as needed.</p>"},{"location":"architecture/backend/#6-error-handling-logging","title":"6. Error Handling &amp; Logging","text":"<ul> <li>Error Handling: Each service uses standard <code>try...catch</code> blocks. On failure, the error is logged to the console, and a standardized JSON error object <code>{ \"error\": \"message\" }</code> is returned to the client with an appropriate HTTP status code (typically <code>400</code> for client errors or <code>500</code> for server errors).</li> <li>Logging: The system uses <code>console.log</code> and <code>console.error</code> for logging. In serverless environments, these outputs are automatically captured by the platform's logging solution (e.g., AWS CloudWatch, Vercel Logs), providing centralized monitoring and debugging capabilities.</li> </ul>"},{"location":"architecture/backend/#7-configuration-environment-variables","title":"7. Configuration &amp; Environment Variables","text":"<p>The backend is configured entirely through environment variables, ensuring no sensitive information is hardcoded.</p> Variable Name Service(s) Used Description <code>GOOGLE_APPLICATION_CREDENTIALS_JSON</code> or <code>GOOGLE_SERVICE_ACCOUNT_KEY</code> Transcription The complete JSON content of the Google Cloud service account key. <code>GEMINI_API_KEY</code> Gemini AI The API key for the Google Gemini service. <code>WEATHER_API_KEY</code> Weather The API key for the WeatherAPI.com service. <code>PORT</code> Local Express Server The port for the local development server to run on. Defaults to <code>3001</code>."},{"location":"architecture/backend/#8-background-jobs-queues","title":"8. Background Jobs &amp; Queues","text":"<p>The system does not currently implement any asynchronous background jobs or message queues. All API requests are processed synchronously.</p>"},{"location":"architecture/backend/#9-testing-qa","title":"9. Testing &amp; QA","text":"<p>The provided codebase does not include automated test suites. For production readiness, the following should be implemented:</p> <ul> <li>Unit Tests: For helper functions like <code>mimeToEncoding</code> and <code>buildSystemInstruction</code> using a framework like Jest or Vitest.</li> <li>Integration Tests: To validate the entire request/response flow of each API endpoint, mocking the external service calls.</li> <li>CI/CD: A pipeline should be configured to automatically run linting and testing on every pull request.</li> </ul>"},{"location":"architecture/backend/#10-deployment","title":"10. Deployment","text":"<p>The backend is architected for flexible deployment. The repository contains detailed guides and configuration files for multiple targets.</p> <ul> <li>**Vercel: The simplest deployment method. The frontend and transcription service are deployed together from a single repository. Configuration is managed by <code>vercel.json</code> and <code>netlify.toml</code>.</li> <li>AWS Lambda: The most robust and scalable option, deploying each service as an independent Lambda function behind an API Gateway. The <code>aws-lambda/DEPLOY.md</code> file contains a comprehensive guide for this setup.</li> <li>**Docker: The <code>Dockerfile</code> allows for packaging the Express server. This container can be deployed to any platform that supports Docker, such as Google Cloud Run, AWS ECS, or a traditional VM.</li> </ul>"},{"location":"architecture/backend/#11-developer-guide","title":"11. Developer Guide","text":""},{"location":"architecture/backend/#111-local-setup-and-onboarding","title":"11.1. Local Setup and Onboarding","text":"<ol> <li>Clone the Repository:     <pre><code>git clone &lt;repository_url&gt;\ncd bean-jam-bot\n</code></pre></li> <li>Install Dependencies:     <pre><code>npm install\n</code></pre></li> <li>Configure Environment:<ul> <li>Create a <code>.env</code> file in the root directory.</li> <li>Copy the contents of <code>.env.example</code> (if available) or add the variables listed in Section 8.</li> <li>For <code>GOOGLE_APPLICATION_CREDENTIALS</code>, provide the local file path to your service account key.</li> </ul> </li> <li>Run the Services:<ul> <li>Terminal 1 (Backend): Start the local transcription server.     <pre><code>npm run start:server\n</code></pre></li> <li>Terminal 2 (Frontend): Start the Vite development server.     <pre><code>npm run dev\n</code></pre></li> </ul> </li> <li>Access the Application: Open <code>http://localhost:8080</code> in your browser.</li> </ol>"},{"location":"architecture/backend/#112-contribution-standards","title":"11.2. Contribution Standards","text":"<ul> <li>Code Style: The project uses ESLint. Please run <code>npm run lint</code> and resolve any issues before submitting code.</li> <li>Branching Strategy: Use a feature-branching workflow.<ul> <li>Create branches from <code>main</code> using a convention like <code>feature/new-chat-ui</code> or <code>bugfix/transcription-error</code>.</li> </ul> </li> <li>Pull Request (PR) Process:<ol> <li>Create your feature branch and commit your changes.</li> <li>Push your branch to the remote repository.</li> <li>Open a Pull Request against the <code>main</code> branch.</li> <li>Provide a clear title and a detailed description of the changes.</li> <li>(Future) Await for CI checks (linting, testing) to pass.</li> <li>Request a review from at least one other team member.</li> <li>Once approved and all checks have passed, the PR can be merged.</li> </ol> </li> </ul>"},{"location":"architecture/diagrams/","title":"Diagrams","text":""},{"location":"architecture/diagrams/#mermaid-diagram-examples","title":"Mermaid Diagram Examples","text":"<p>This page demonstrates various Mermaid diagrams supported in the documentation.</p>"},{"location":"architecture/diagrams/#system-architecture","title":"System Architecture","text":"<pre><code>graph TB\n    User[User Browser] --&gt; React[React Frontend]\n    React --&gt; Lambda[AWS Lambda Functions]\n    Lambda --&gt; Gemini[Google Gemini API]\n    Lambda --&gt; Speech[Google Speech-to-Text]\n    Lambda --&gt; Weather[WeatherAPI.com]\n    React --&gt; Location[Location Services]\n    Location --&gt; GPS[GPS/Geolocation]\n    Location --&gt; IP[IP-based Location]</code></pre>"},{"location":"architecture/diagrams/#request-flow","title":"Request Flow","text":"<pre><code>sequenceDiagram\n    participant U as User\n    participant F as Frontend\n    participant L as Lambda\n    participant G as Gemini AI\n\n    U-&gt;&gt;F: Voice Input\n    F-&gt;&gt;L: Transcribe Audio\n    L-&gt;&gt;F: Return Text\n    F-&gt;&gt;L: Send Message + Context\n    L-&gt;&gt;G: Generate Response\n    G-&gt;&gt;L: AI Response\n    L-&gt;&gt;F: Formatted Response\n    F-&gt;&gt;U: Display + Weather Card</code></pre>"},{"location":"architecture/diagrams/#component-structure","title":"Component Structure","text":"<pre><code>graph LR\n    Index[Index Page] --&gt; Blob[BlobAnimation]\n    Index --&gt; Chat[ChatMessage]\n    Index --&gt; Lang[LanguageToggle]\n    Index --&gt; Weather[WeatherCard]\n    Index --&gt; Hooks[Custom Hooks]\n    Hooks --&gt; Audio[use-audio-recorder]\n    Hooks --&gt; Level[use-audio-level]</code></pre>"},{"location":"architecture/diagrams/#deployment-pipeline","title":"Deployment Pipeline","text":"<pre><code>graph LR\n    A[Code Push] --&gt; B[GitHub]\n    B --&gt; C[GitHub Actions]\n    C --&gt; D{Build Success?}\n    D --&gt;|Yes| E[Deploy to GitHub Pages]\n    D --&gt;|No| F[Notify Developer]\n    E --&gt; G[Live Documentation]</code></pre>"},{"location":"architecture/diagrams/#state-management","title":"State Management","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; Idle\n    Idle --&gt; Recording: Start Recording\n    Recording --&gt; Processing: Stop Recording\n    Processing --&gt; Transcribing: Upload Audio\n    Transcribing --&gt; Responding: Send to Gemini\n    Responding --&gt; DisplayResult: Receive Response\n    DisplayResult --&gt; Idle: Complete\n    Recording --&gt; Idle: Cancel\n    Processing --&gt; Idle: Error</code></pre>"},{"location":"architecture/diagrams/#api-integration-flow","title":"API Integration Flow","text":"<pre><code>flowchart TD\n    Start([User Interaction]) --&gt; Input{Input Type?}\n    Input --&gt;|Text| TextMsg[Text Message]\n    Input --&gt;|Voice| VoiceRec[Voice Recording]\n\n    VoiceRec --&gt; Transcribe[AWS Lambda Transcribe]\n    Transcribe --&gt; TextMsg\n\n    TextMsg --&gt; Location[Get Location]\n    Location --&gt; Weather[Fetch Weather Data]\n    Weather --&gt; Context[Build Context]\n\n    Context --&gt; Gemini[Send to Gemini AI]\n    Gemini --&gt; Parse[Parse Response]\n    Parse --&gt; Display[Display to User]\n    Display --&gt; End([Complete])</code></pre>"},{"location":"architecture/diagrams/#tech-stack","title":"Tech Stack","text":"<pre><code>mindmap\n  root((Bean Jam Bot))\n    Frontend\n      React 18\n      TypeScript\n      Vite\n      Tailwind CSS\n      Framer Motion\n    Backend\n      AWS Lambda\n      Node.js 20.x\n      Serverless\n    APIs\n      Google Gemini Pro\n      Google Speech-to-Text\n      WeatherAPI.com\n      OpenStreetMap\n    Features\n      Voice Recognition\n      Bilingual Support\n      Weather Integration\n      Location Services</code></pre>"},{"location":"architecture/diagrams/#class-diagram","title":"Class Diagram","text":"<pre><code>classDiagram\n    class GeminiService {\n        -apiKey: string\n        -model: GenerativeModel\n        +generateResponse()\n        +sanitizeMarkdown()\n    }\n\n    class LocationService {\n        +getUserLocation()\n        +getGPSLocation()\n        +getIPLocation()\n    }\n\n    class WeatherService {\n        +getWeather()\n        +formatWeatherSummary()\n        +extractWeatherCardData()\n    }\n\n    class AudioRecorder {\n        -mediaRef: MediaRecorder\n        +start()\n        +stop()\n        +sendToServer()\n    }\n\n    GeminiService --&gt; WeatherService: uses\n    GeminiService --&gt; LocationService: uses\n    AudioRecorder --&gt; GeminiService: transcribed text</code></pre>"},{"location":"architecture/diagrams/#gantt-chart-development-timeline","title":"Gantt Chart - Development Timeline","text":"<pre><code>gantt\n    title Bean Jam Bot Development\n    dateFormat  YYYY-MM-DD\n    section Phase 1\n    Core Chat Interface           :2025-01-01, 7d\n    Gemini Integration           :2025-01-08, 5d\n    section Phase 2\n    Voice Recording              :2025-01-13, 7d\n    Speech-to-Text              :2025-01-20, 5d\n    section Phase 3\n    Location Services            :2025-01-25, 5d\n    Weather Integration          :2025-01-30, 5d\n    section Phase 4\n    Bilingual Support            :2025-02-04, 7d\n    UI Polish                    :2025-02-11, 7d\n    section Deployment\n    AWS Lambda Setup             :2025-02-18, 5d\n    Production Deployment        :2025-02-23, 3d</code></pre>"},{"location":"architecture/diagrams/#entity-relationship","title":"Entity Relationship","text":"<pre><code>erDiagram\n    USER ||--o{ MESSAGE : sends\n    MESSAGE ||--|| RESPONSE : receives\n    RESPONSE ||--o| WEATHER_CARD : includes\n    USER ||--|| LOCATION : has\n    LOCATION ||--|| WEATHER : fetches\n\n    USER {\n        string language\n        array messages\n        boolean isRecording\n    }\n\n    MESSAGE {\n        string content\n        boolean isUser\n        datetime timestamp\n    }\n\n    RESPONSE {\n        string text\n        boolean showWeatherCard\n    }\n\n    WEATHER_CARD {\n        string location\n        float temperature\n        string condition\n        int humidity\n    }</code></pre> <p>Note: All these diagrams are rendered using Mermaid.js, which is now fully supported in the documentation!</p>"},{"location":"architecture/frontend/","title":"Frontend","text":""},{"location":"architecture/frontend/#bean-jam-bot-technical-documentation","title":"Bean Jam Bot - Technical Documentation","text":""},{"location":"architecture/frontend/#1-project-overview","title":"1. Project Overview","text":""},{"location":"architecture/frontend/#11-purpose","title":"1.1. Purpose","text":"<p>Bean Jam Bot is an enterprise-grade, bilingual (English/Japanese) AI chat assistant designed for date and restaurant planning. It features a modern, \"brutalist\" user interface with sophisticated voice input capabilities. The application leverages a suite of Google Cloud AI services to provide natural, context-aware interactions, including location-based weather information to enhance its recommendations.</p>"},{"location":"architecture/frontend/#12-architecture-summary","title":"1.2. Architecture Summary","text":"<p>The system is architected as a modern JAMstack application. The frontend is a single-page application (SPA) built with React and Vite. All backend logic, including API key management and communication with third-party services, is handled by serverless functions. This decoupling ensures security, scalability, and platform flexibility.</p> <p>The architecture supports multiple deployment targets, including Vercel, Netlify, and AWS Lambda, with configurations provided for each. This allows the application to be deployed across various cloud infrastructures seamlessly.</p>"},{"location":"architecture/frontend/#13-technology-stack","title":"1.3. Technology Stack","text":"Category Technology Core Framework React 18, TypeScript Build Tool Vite State Management React Context API (for language), TanStack Query (for server state caching), Component State (<code>useState</code>) Routing React Router v6 Styling Tailwind CSS, <code>tailwindcss-animate</code> UI Components <code>shadcn/ui</code> (built on Radix UI) API Services \u2022 AI Chat: Google Gemini (<code>gemini-2.5-flash</code>)  \u2022 Speech-to-Text: Google Cloud Speech-to-Text  \u2022 Weather: WeatherAPI.com  \u2022 Geolocation: Browser Geolocation API, ipapi.co (fallback) Backend Serverless Functions (Node.js) on Vercel, Netlify, or AWS Lambda; Express.js for local development. Deployment Vercel, Netlify, AWS, Docker Linting ESLint with TypeScript ESLint"},{"location":"architecture/frontend/#2-code-structure-overview","title":"2. Code Structure Overview","text":"<p>The repository is organized to separate concerns between the frontend application, serverless functions for different platforms, and configuration files.</p> <pre><code>/\n\u251c\u2500\u2500 api/                    # Vercel Serverless Functions\n\u2502   \u2514\u2500\u2500 transcribe.js\n\u251c\u2500\u2500 aws-lambda/             # AWS Lambda Functions (Gemini, Transcribe, Weather)\n\u2502   \u251c\u2500\u2500 gemini/\n\u2502   \u251c\u2500\u2500 transcribe/\n\u2502   \u2514\u2500\u2500 weather/\n\u251c\u2500\u2500 netlify/                # Netlify Serverless Functions\n\u2502   \u2514\u2500\u2500 functions/\n\u2502       \u2514\u2500\u2500 transcribe.js\n\u251c\u2500\u2500 public/                 # Static assets (icons, robots.txt)\n\u251c\u2500\u2500 server/                 # Local Express.js server for development proxy\n\u2502   \u2514\u2500\u2500 index.js\n\u251c\u2500\u2500 src/                    # Main frontend application source\n\u2502   \u251c\u2500\u2500 components/         # Reusable React components\n\u2502   \u2502   \u251c\u2500\u2500 ui/             # shadcn/ui components\n\u2502   \u2502   \u251c\u2500\u2500 BlobAnimation.tsx # Core visual feedback component\n\u2502   \u2502   \u251c\u2500\u2500 ChatMessage.tsx   # Renders a single chat message\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 contexts/           # React Context providers (e.g., LanguageContext)\n\u2502   \u251c\u2500\u2500 hooks/              # Custom React hooks\n\u2502   \u251c\u2500\u2500 lib/                # Core business logic, API clients, and utilities\n\u2502   \u251c\u2500\u2500 pages/              # Top-level route components\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 Dockerfile              # Docker configuration for containerization\n\u251c\u2500\u2500 netlify.toml            # Netlify deployment configuration\n\u251c\u2500\u2500 package.json            # Project dependencies and scripts\n\u251c\u2500\u2500 tailwind.config.ts      # Tailwind CSS configuration\n\u251c\u2500\u2500 tsconfig.json           # TypeScript configuration\n\u251c\u2500\u2500 vercel.json             # Vercel deployment configuration\n\u2514\u2500\u2500 vite.config.ts          # Vite build configuration\n</code></pre>"},{"location":"architecture/frontend/#3-core-modules-components","title":"3. Core Modules &amp; Components","text":""},{"location":"architecture/frontend/#31-pages","title":"3.1. Pages","text":""},{"location":"architecture/frontend/#srcpagesindextsx","title":"<code>src/pages/Index.tsx</code>","text":"<p>This is the main component, orchestrating the entire chat experience.</p> <ul> <li>State Management: Manages chat messages, user input, loading states (<code>isLoading</code>, <code>isCreating</code>), and user location via <code>useState</code> and <code>useRef</code>.</li> <li>Core Logic:<ul> <li>Handles form submission for text-based messages.</li> <li>Integrates <code>useAudioRecorder</code> and <code>useAudioLevel</code> hooks to manage voice input, recording state, and visual feedback.</li> <li>Coordinates API calls to the Gemini service for generating responses.</li> <li>Manages the display of the <code>WeatherCard</code> based on Gemini's response.</li> </ul> </li> <li>Dependencies: <code>BlobAnimation</code>, <code>ChatMessage</code>, <code>LanguageToggle</code>, <code>WeatherCard</code>, <code>useAudioRecorder</code>, <code>useAudioLevel</code>, <code>getGeminiService</code>, <code>getUserLocation</code>, <code>getWeather</code>.</li> </ul>"},{"location":"architecture/frontend/#srcpagesnotfoundtsx","title":"<code>src/pages/NotFound.tsx</code>","text":"<p>A standard 404 page displayed for any route not explicitly defined.</p>"},{"location":"architecture/frontend/#32-custom-components","title":"3.2. Custom Components","text":""},{"location":"architecture/frontend/#srccomponentsblobanimationtsx","title":"<code>src/components/BlobAnimation.tsx</code>","text":"<p>A critical UI component providing visual feedback to the user.</p> <ul> <li>Props:<ul> <li><code>isLoading: boolean</code>: Indicates a pending operation (recording or waiting for AI).</li> <li><code>isBehind: boolean</code>: Renders the animation behind the chat UI if <code>true</code>.</li> <li><code>mode: 'normal' | 'create'</code>: Alters the color and intensity of the animation.</li> <li><code>levels: Float32Array | null</code>: Real-time audio frequency data to drive animation responsiveness.</li> </ul> </li> <li>Functionality: Uses <code>requestAnimationFrame</code> to create a dynamic, \"gooey\" blob effect that reacts to audio input and loading states, enhancing user engagement.</li> </ul>"},{"location":"architecture/frontend/#srccomponentschatmessagetsx","title":"<code>src/components/ChatMessage.tsx</code>","text":"<p>Renders a single message bubble in the chat interface.</p> <ul> <li>Props:<ul> <li><code>content: string</code>: The text content of the message.</li> <li><code>isUser: boolean</code>: Determines styling (e.g., alignment, color).</li> <li><code>language: 'en' | 'jp'</code>: Used for language-specific rendering if needed.</li> </ul> </li> <li>Functionality: Includes a <code>formatMessageContent</code> utility to parse and render Gemini's markdown-like responses into structured HTML (lists, paragraphs).</li> </ul>"},{"location":"architecture/frontend/#srccomponentsweathercardtsx","title":"<code>src/components/WeatherCard.tsx</code>","text":"<p>A dedicated component to display weather information.</p> <ul> <li>Props: A <code>WeatherCardProps</code> object containing <code>location</code>, <code>temperature</code>, <code>condition</code>, <code>humidity</code>, etc.</li> <li>Functionality: Renders a \"brutalist\"-styled card with formatted weather data. It uses the <code>LanguageContext</code> to display labels in either English or Japanese.</li> </ul>"},{"location":"architecture/frontend/#33-hooks","title":"3.3. Hooks","text":""},{"location":"architecture/frontend/#srchooksuse-audio-recorderts","title":"<code>src/hooks/use-audio-recorder.ts</code>","text":"<p>Encapsulates all logic for recording audio from the user's microphone.</p> <ul> <li>Exports: <code>start</code>, <code>stop</code>, <code>sendToServer</code>, and <code>isRecording</code>.</li> <li>Browser Compatibility: Intelligently detects the best supported audio MIME type (<code>audio/webm</code>, <code>audio/ogg</code>, or <code>audio/wav</code>) to ensure cross-browser functionality.</li> <li>Functionality: Manages <code>MediaRecorder</code> state, captures audio into a <code>Blob</code>, and provides a function to send the base64-encoded audio to the transcription backend.</li> </ul>"},{"location":"architecture/frontend/#srchooksuse-audio-levelts","title":"<code>src/hooks/use-audio-level.ts</code>","text":"<p>Provides real-time audio frequency data from the microphone.</p> <ul> <li>Functionality: Uses the Web Audio API (<code>AudioContext</code>, <code>AnalyserNode</code>) to capture microphone input levels. The output is consumed by <code>BlobAnimation</code> for visualization.</li> </ul>"},{"location":"architecture/frontend/#34-libraries-utilities-srclib","title":"3.4. Libraries &amp; Utilities (<code>src/lib</code>)","text":"<ul> <li><code>gemini.ts</code>: A service class that abstracts communication with the Google Gemini API. It constructs the request payload, including system instructions and conversation history, and can route requests through a serverless proxy to protect the API key.</li> <li><code>weather.ts</code>: A service for fetching data from WeatherAPI.com. It includes logic to format requests based on user location and parse the response into structured data, including the <code>WeatherCardData</code> format.</li> <li><code>location.ts</code>: Manages user location detection. It first attempts to use the high-accuracy browser Geolocation API (GPS) and gracefully falls back to a lower-accuracy IP-based location service (<code>ipapi.co</code>) if GPS is unavailable or denied.</li> <li><code>browser-compat.ts</code>: A crucial utility module that detects browser features (<code>MediaRecorder</code>, <code>getUserMedia</code>, etc.) and provides user-friendly error messages if a feature is unsupported. This enables graceful degradation for older browsers.</li> <li><code>utils.ts</code>: Contains the <code>cn</code> utility for merging Tailwind CSS classes and a <code>sanitizeMarkdown</code> function to strip markdown syntax from AI responses.</li> </ul>"},{"location":"architecture/frontend/#4-state-management","title":"4. State Management","text":"<p>The application employs a hybrid state management strategy suitable for its complexity.</p> <ul> <li>Global State: A single global state, Language, is managed via React's Context API (<code>src/contexts/LanguageContext.tsx</code>). This allows any component to access the current language (<code>en</code> or <code>jp</code>) and toggle it.</li> <li>Server State &amp; Caching: TanStack Query (<code>@tanstack/react-query</code>) is integrated at the root of the application (<code>App.tsx</code>). While the primary chat flow uses direct <code>fetch</code> calls, TanStack Query is available for caching, refetching, and managing the state of asynchronous data from backend services.</li> <li>Component State: The majority of the application's state (chat history, input values, loading status) is managed locally within the <code>Index.tsx</code> page using React hooks (<code>useState</code>, <code>useRef</code>). This colocation of state with its view logic simplifies the data flow for the application's primary feature.</li> </ul>"},{"location":"architecture/frontend/#5-routing-navigation","title":"5. Routing &amp; Navigation","text":"<p>Routing is handled by React Router v6. The configuration in <code>src/App.tsx</code> is straightforward:</p> <ul> <li><code>/</code>: Maps to the main <code>Index</code> page component.</li> <li><code>*</code>: A catch-all route that directs any unmatched URL to the <code>NotFound</code> page.</li> </ul> <p>The application does not currently implement protected routes or complex lazy-loading, as its primary functionality is contained within a single view.</p>"},{"location":"architecture/frontend/#6-styling-system","title":"6. Styling System","text":"<p>The UI is built on a robust and modern styling system.</p> <ul> <li>Framework: Tailwind CSS is used for all styling, enabling a utility-first workflow. The configuration is located in <code>tailwind.config.ts</code>.</li> <li>Component Library: The project uses <code>shadcn/ui</code>, a collection of accessible and composable components built on Radix UI and styled with Tailwind CSS. These components are not imported as a library but are co-located within the codebase at <code>src/components/ui/</code>, allowing for full customization.</li> <li>Theming: A theming system based on CSS variables is defined in <code>src/index.css</code>, following <code>shadcn/ui</code> conventions. This allows for centralized control over the color palette, typography (<code>IBM Plex Mono</code>), and other design tokens. The aesthetic is intentionally \"brutalist,\" characterized by sharp edges, high contrast, and a monospaced font.</li> <li>Responsive Design: The utility-first nature of Tailwind CSS facilitates a fully responsive, mobile-first design.</li> </ul>"},{"location":"architecture/frontend/#7-apis-and-data-handling","title":"7. APIs and Data Handling","text":"<p>The frontend interacts with backend services through a secure proxy layer implemented as serverless functions. This prevents exposure of sensitive API keys on the client-side.</p> <ul> <li>Transcription API (<code>/api/transcribe</code>):<ul> <li>The frontend captures audio, encodes it as a base64 string, and sends it along with its MIME type to this endpoint.</li> <li>The serverless function forwards the request to the Google Cloud Speech-to-Text API and returns the transcript.</li> </ul> </li> <li>Gemini API (via <code>GeminiService</code>):<ul> <li>The <code>GeminiService</code> constructs a detailed prompt including the user's message, conversation history, language preference, and contextual data (location, weather).</li> <li>It sends this payload to a serverless proxy (<code>VITE_GEMINI_PROXY_URL</code>) or directly to the Gemini API if no proxy is configured.</li> <li>The service parses the response, checking for a special <code>[SHOW_WEATHER_CARD]</code> marker that dictates whether the weather card should be displayed.</li> </ul> </li> <li>Weather API (via <code>getWeather</code>):<ul> <li>Fetches weather data from a proxy (<code>VITE_WEATHER_PROXY_URL</code>) which in turn calls WeatherAPI.com. This keeps the Weather API key secure.</li> </ul> </li> <li>Error Handling: All asynchronous operations are wrapped in <code>try...catch</code> blocks. User-facing errors are displayed using the custom <code>useToast</code> hook, providing clear and non-disruptive feedback.</li> <li>Loading States: The UI provides comprehensive feedback for all loading states. The <code>isLoading</code> and <code>isCreating</code> flags control the <code>BlobAnimation</code> and disable UI elements to prevent duplicate submissions.</li> </ul>"},{"location":"architecture/frontend/#8-testing-strategy","title":"8. Testing Strategy","text":"<p>The current version of the codebase does not have an integrated testing framework (<code>Jest</code>, <code>Vitest</code>, <code>Cypress</code>, etc.) configured in <code>package.json</code>.</p> <p>Recommendation: For an enterprise-level system, implementing a comprehensive testing strategy is a critical next step. The recommended approach would be: *   Unit &amp; Integration Tests: Use Vitest with React Testing Library to test individual components, hooks, and utility functions in isolation. *   End-to-End (E2E) Tests: Use Cypress or Playwright to test critical user flows, such as submitting a text message, completing a voice recording session, and verifying the AI response. *   Code Coverage: Aim for a minimum of 80% test coverage across all critical modules.</p>"},{"location":"architecture/frontend/#10-developer-onboarding-guide","title":"10. Developer Onboarding Guide","text":""},{"location":"architecture/frontend/#101-welcome","title":"10.1. Welcome","text":"<p>Welcome to the Bean Jam Bot project! This guide will help you get your development environment set up and running quickly.</p>"},{"location":"architecture/frontend/#102-prerequisites","title":"10.2. Prerequisites","text":"<ol> <li>Node.js: Version 18.x or later.</li> <li>API Keys: You will need to obtain the following API keys:<ul> <li>Google Cloud Service Account JSON (for Speech-to-Text).</li> <li>Google Gemini API Key.</li> <li>WeatherAPI.com API Key.</li> </ul> </li> <li>Vercel CLI (Optional): <code>npm install -g vercel</code> for easy deployment.</li> </ol>"},{"location":"architecture/frontend/#103-local-development-setup","title":"10.3. Local Development Setup","text":"<ol> <li> <p>Clone the Repository:     </p><pre><code>git clone &lt;repository-url&gt;\ncd bean-jam-bot\n</code></pre><p></p> </li> <li> <p>Install Dependencies:     </p><pre><code>npm install\n</code></pre><p></p> </li> <li> <p>Configure Environment Variables:     Create a new file named <code>.env</code> in the root of the project and add your API keys. Use the <code>.gitignore</code> file as a reference for the variable names. It should look like this:     </p><pre><code># Path to your Google Cloud service account JSON file\nGOOGLE_APPLICATION_CREDENTIALS=./src/Keys/your-service-account-file.json\n\n# API key for Google Gemini\nVITE_GEMINI_API_KEY=AIzaSy...\n\n# API key for WeatherAPI.com\nVITE_WEATHER_API_KEY=your_weather_api_key\n</code></pre> Note: Place your Google Cloud JSON key file in the <code>src/Keys/</code> directory.<p></p> </li> <li> <p>Run the Application:     You need to run two processes in separate terminals:</p> <ul> <li>Terminal 1 (Backend Server): Starts the local Express.js proxy for transcription.     <pre><code>npm run start:server\n</code></pre></li> <li>Terminal 2 (Frontend Server): Starts the Vite development server.     <pre><code>npm run dev\n</code></pre></li> </ul> </li> <li> <p>Access the Application: Open your browser and navigate to <code>http://localhost:8080</code>.</p> </li> </ol>"},{"location":"architecture/frontend/#104-key-files-and-modules","title":"10.4. Key Files and Modules","text":"<ul> <li>Main UI Logic: <code>src/pages/Index.tsx</code> - This is the heart of the application.</li> <li>Voice Recording: <code>src/hooks/use-audio-recorder.ts</code> - All microphone and recording logic lives here.</li> <li>AI Integration: <code>src/lib/gemini.ts</code> - This service class handles all interactions with the Gemini AI.</li> <li>Backend Proxy (Local): <code>server/index.js</code> - The Express server that runs locally to proxy transcription requests.</li> <li>Backend Proxy (Production): <code>api/transcribe.js</code> (Vercel), <code>netlify/functions/transcribe.js</code> (Netlify), <code>aws-lambda/transcribe/index.js</code> (AWS).</li> </ul>"},{"location":"architecture/frontend/#105-contribution-workflow","title":"10.5. Contribution Workflow","text":"<ol> <li>Create a new feature branch from <code>main</code>.</li> <li>Implement your changes.</li> <li>(Action Required): Add relevant unit or integration tests for your changes using Vitest and React Testing Library.</li> <li>Ensure the linter passes: <code>npm run lint</code>.</li> <li>Submit a Pull Request for review.</li> </ol>"},{"location":"architecture/system-design/","title":"System Design","text":""},{"location":"architecture/system-design/#system-architecture-design-bean-jam-bot","title":"System Architecture &amp; Design: Bean Jam Bot","text":"<p>Version: 1.0 Date: October 18, 2023 Author: System Architect AI</p>"},{"location":"architecture/system-design/#1-system-overview","title":"1. System Overview","text":"<p>Bean Jam Bot is a modern, full-stack AI chat assistant designed for date and restaurant planning. It provides a conversational interface with bilingual (English/Japanese) support, leveraging voice-to-text transcription for user input. The architecture is built on a serverless-first, multi-cloud model, prioritizing scalability, maintainability, and security by isolating third-party API interactions into a dedicated backend layer.</p> <p>The system's core purpose is to offer users intelligent recommendations by integrating a powerful language model (Google Gemini) with real-time, location-aware data services like weather and geolocation. The frontend is a responsive React/Vite single-page application (SPA), while the backend consists of a suite of independent, single-purpose serverless functions acting as secure proxies to external APIs.</p> <p>High-Level Technologies:</p> <ul> <li>Frontend: React, Vite, TypeScript, TailwindCSS</li> <li>Backend: Node.js, Serverless Functions (AWS Lambda, Vercel Functions, Netlify Functions)</li> <li>AI &amp; ML Services: Google Gemini (Chat), Google Cloud Speech-to-Text (Transcription)</li> <li>Third-Party Services: WeatherAPI.com (Weather Data), OpenStreetMap (Reverse Geocoding)</li> <li>Deployment: Vercel, Netlify, AWS, Docker</li> </ul>"},{"location":"architecture/system-design/#2-architecture-diagram-data-flow","title":"2. Architecture Diagram &amp; Data Flow","text":"<p>The system follows a decoupled, microservices-like architecture where the frontend orchestrates calls to various specialized serverless backends. This design ensures that sensitive API keys are never exposed to the client. ![[Pasted image 20251018132113.png]]</p>"},{"location":"architecture/system-design/#data-flow-description","title":"Data Flow Description:","text":"<ol> <li> <p>Voice Transcription:</p> <ul> <li>The user initiates voice input on the React SPA. The browser's <code>MediaRecorder</code> API captures audio.</li> <li>The captured audio is Base64 encoded and sent via an HTTPS POST request to the Transcribe API (<code>/api/transcribe</code>).</li> <li>The serverless function receives the audio data and securely calls the Google Speech-to-Text API.</li> <li>Google's API returns the transcript, which the serverless function relays back to the frontend.</li> </ul> </li> <li> <p>Chat &amp; Weather Integration:</p> <ul> <li>The frontend first attempts to get the user's location via the browser's Geolocation API, with fallback to an IP-based lookup. If GPS coordinates are obtained, it uses OpenStreetMap for reverse geocoding.</li> <li>If location is available, the frontend calls the Weather Proxy API to fetch current weather from WeatherAPI.com.</li> <li>The user's message, conversation history, and the fetched weather data (as a string summary) are sent in a single POST request to the Gemini Proxy API.</li> <li>The Gemini proxy constructs a detailed prompt and securely calls the Google Gemini API.</li> <li>Gemini's response, which may include a special <code>[SHOW_WEATHER_CARD]</code> marker, is returned to the frontend.</li> <li>The frontend parses the response, displaying the text and a weather card if the marker is present.</li> </ul> </li> </ol>"},{"location":"architecture/system-design/#3-frontend-backend-interface","title":"3. Frontend-Backend Interface","text":"<p>The interface between the frontend and backend is a set of RESTful APIs served by serverless functions. Communication is exclusively over HTTPS using JSON payloads.</p> <ul> <li>Transcription Endpoint: <code>POST /api/transcribe</code><ul> <li>Request Body: <code>{ audioBase64: string, mimeType: string, languageCode: 'en-US' | 'ja-JP' }</code></li> <li>Response Body: <code>{ transcript: string, raw: object }</code></li> </ul> </li> <li>Gemini Chat Endpoint: <code>POST /api/gemini</code> (via proxy)<ul> <li>Request Body: <code>{ message: string, language: 'en' | 'jp', conversationHistory: Message[], userLocation?: string, weatherData?: string }</code></li> <li>Response Body: <code>{ response: string, showWeatherCard: boolean }</code></li> </ul> </li> <li>Weather Endpoint: <code>POST /api/weather</code> (via proxy)<ul> <li>Request Body: <code>{ q: string, days: number }</code></li> <li>Response Body: WeatherAPI.com forecast JSON object.</li> </ul> </li> </ul> <p>Latency Optimization: *   The frontend provides immediate user feedback with loading indicators and animations (<code>isLoading</code>, <code>BlobAnimation</code>). *   The entire conversation history is passed in each request, making the backend stateless but increasing payload size. *   Serverless \"cold starts\" may introduce initial latency, but subsequent requests are fast.</p>"},{"location":"architecture/system-design/#4-infrastructure-deployment","title":"4. Infrastructure &amp; Deployment","text":"<p>The system is designed with a flexible, multi-cloud deployment strategy, leveraging serverless computing to minimize operational overhead and scale on demand.</p> <ul> <li>Hosting Model: The primary model is serverless, with documented support for Vercel, Netlify, and AWS Lambda + API Gateway. This avoids vendor lock-in and allows deployment on the platform that best fits the operational need.</li> <li>Scaling: All backend services (transcription, chat, weather) are hosted on serverless platforms, which automatically scale horizontally based on incoming traffic. This is a highly cost-effective and resilient approach.</li> <li>Monitoring &amp; Logging:<ul> <li>AWS: Detailed execution logs, metrics (invocations, duration, errors), and traces are available via AWS CloudWatch.</li> <li>Vercel/Netlify: Provide built-in dashboards for function logs, invocation analytics, and performance monitoring.</li> <li>Frontend: No explicit client-side logging service is integrated, but browser console logs provide debugging information.</li> </ul> </li> <li>CI/CD: The repository includes a PowerShell script (<code>deploy-prod.ps1</code>) for streamlined Vercel deployments. However, a formal CI/CD pipeline (e.g., GitHub Actions) is not yet configured.</li> </ul>"},{"location":"architecture/system-design/#5-security-compliance","title":"5. Security &amp; Compliance","text":"<p>Security is architected around the principle of least privilege and protecting sensitive credentials.</p> <ul> <li>Data Protection: All API keys and the Google Cloud Service Account JSON are stored as environment variables in the secure configuration of the deployment platform (Vercel, Netlify, AWS). They are never exposed to the frontend client.</li> <li>Encryption: All data in transit between the client, backend functions, and third-party APIs is encrypted via HTTPS/TLS.</li> <li>Authentication: The system currently operates without user authentication, making it a public-facing service. Access is not restricted.</li> <li>Backend for Frontend (BFF): The serverless functions act as a BFF/Proxy layer, abstracting the complex and sensitive interactions with third-party services from the client.</li> </ul>"},{"location":"architecture/system-design/#6-performance-reliability","title":"6. Performance &amp; Reliability","text":"<p>The serverless architecture provides a strong foundation for reliability and performance.</p> <ul> <li>Load Balancing: Handled natively by the serverless provider (AWS ALB, Vercel/Netlify edge network), distributing traffic across function instances.</li> <li>Redundancy &amp; Failure Recovery: Serverless functions are inherently fault-tolerant. If an instance fails, the provider automatically provisions a new one to handle subsequent requests. The frontend includes error handling to display toast notifications if an API call fails.</li> <li>SLA Considerations: The overall system uptime is a composite of the SLAs of the frontend host (e.g., Vercel) and the third-party APIs (Google Cloud, WeatherAPI.com). High availability is expected due to the reliance on major cloud providers.</li> </ul>"},{"location":"architecture/system-design/#7-scalability-extensibility","title":"7. Scalability &amp; Extensibility","text":"<p>The architecture is highly modular and designed for growth.</p> <ul> <li>Scalability: The use of independent serverless functions for distinct business logic (transcribe, chat, weather) allows each component to scale independently based on its specific load.</li> <li>Extensibility: Adding new functionality is straightforward. For example, to add image analysis, a new serverless function (<code>/api/analyze-image</code>) could be created and called from the frontend without modifying any existing services. The component-based React architecture on the frontend also allows for easy addition of new UI features.</li> </ul>"},{"location":"architecture/system-design/#8-dependencies-integrations","title":"8. Dependencies &amp; Integrations","text":"<p>The system relies on several key external services to deliver its functionality.</p> <ul> <li>Google Cloud Speech-to-Text: Core service for voice input transcription.</li> <li>Google Gemini API: Core service for generating intelligent, conversational responses.</li> <li>WeatherAPI.com: Provides real-time and forecast weather data.</li> <li>OpenStreetMap Nominatim: A free service used for reverse geocoding to convert GPS coordinates into human-readable city/region names.</li> </ul>"},{"location":"architecture/system-design/#9-future-recommendations","title":"9. Future Recommendations","text":"<p>While the current architecture is robust and scalable, the following improvements could enhance it further:</p> <ol> <li> <p>State Management &amp; Caching:</p> <ul> <li>Implement Server-Side State: Instead of passing conversation history on every request, store it server-side using a low-latency database like Redis or DynamoDB, linked to a user session. This will reduce request payload size and improve performance for long conversations.</li> <li>Cache Weather Data: Cache weather API responses for a short TTL (e.g., 15 minutes) based on location query to reduce redundant API calls and lower costs.</li> </ul> </li> <li> <p>User Authentication &amp; Authorization:</p> <ul> <li>Integrate an authentication provider (e.g., Auth0, Firebase Auth) to manage users.</li> <li>This would enable personalized experiences, persistent chat history, and the ability to implement rate limiting on a per-user basis to prevent abuse.</li> </ul> </li> <li> <p>Real-Time Experience:</p> <ul> <li>For voice input, migrate from a \"record-then-send\" model to a streaming model using WebSockets and the Google Speech-to-Text streaming API. This would provide real-time transcription and a more interactive user experience.</li> </ul> </li> <li> <p>Infrastructure Consolidation &amp; CI/CD:</p> <ul> <li>While multi-platform support is flexible, for a production system, standardizing on a single cloud provider (e.g., AWS for all functions) would simplify IAM, security policies, and monitoring.</li> <li>Implement a formal CI/CD pipeline using GitHub Actions to automate testing, building, and deploying both the frontend and serverless functions.</li> </ul> </li> <li> <p>Enhanced Observability:</p> <ul> <li>Integrate a dedicated logging and error tracking service like Sentry or Datadog for both frontend and backend to proactively monitor for issues in production.</li> </ul> </li> </ol>"},{"location":"deployment/aws-lambda/","title":"AWS Lambda","text":""},{"location":"deployment/aws-lambda/#deploy-bean-jam-transcribe-lambda-to-aws","title":"Deploy Bean Jam Transcribe Lambda to AWS","text":""},{"location":"deployment/aws-lambda/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Account with CLI configured (<code>aws configure</code>)</li> <li>AWS IAM permissions to create Lambda functions, API Gateway, and IAM roles</li> <li>Google Cloud service account JSON for Speech-to-Text API</li> </ul>"},{"location":"deployment/aws-lambda/#step-1-create-iam-role-for-lambda","title":"Step 1: Create IAM Role for Lambda","text":"<ol> <li>AWS Console: IAM \u2192 Roles \u2192 Create role</li> <li>Trusted entity: AWS service \u2192 Lambda</li> <li>Permissions: Attach these policies:<ul> <li><code>AWSLambdaBasicExecutionRole</code> (CloudWatch Logs)</li> <li>Custom inline policy for Secrets Manager (optional, if storing credentials there)</li> </ul> </li> <li>Role name: <code>BeanJamTranscribeLambdaRole</code></li> <li>Copy the Role ARN (e.g., <code>arn:aws:iam::123456789012:role/BeanJamTranscribeLambdaRole</code>)</li> </ol> <p>Or via CLI: </p><pre><code># Create trust policy\ncat &gt; trust-policy.json &lt;&lt; 'EOF'\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [{\n    \"Effect\": \"Allow\",\n    \"Principal\": {\"Service\": \"lambda.amazonaws.com\"},\n    \"Action\": \"sts:AssumeRole\"\n  }]\n}\nEOF\n\n# Create role\naws iam create-role \\\n  --role-name BeanJamTranscribeLambdaRole \\\n  --assume-role-policy-document file://trust-policy.json\n\n# Attach basic execution policy\naws iam attach-role-policy \\\n  --role-name BeanJamTranscribeLambdaRole \\\n  --policy-arn arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\n</code></pre><p></p>"},{"location":"deployment/aws-lambda/#step-2-package-the-lambda-function","title":"Step 2: Package the Lambda Function","text":"<pre><code># Navigate to Lambda directory\ncd aws-lambda/transcribe\n\n# Install dependencies\nnpm install --production\n\n# Create deployment package (ZIP)\n# Windows (PowerShell with 7-Zip or native Compress-Archive):\nCompress-Archive -Path * -DestinationPath function.zip -Force\n\n# Or use AWS SAM CLI (recommended):\n# sam build &amp;&amp; sam package\n</code></pre>"},{"location":"deployment/aws-lambda/#step-3-create-lambda-function","title":"Step 3: Create Lambda Function","text":"<p>Option A: AWS Console 1. Lambda \u2192 Create function    - Function name: <code>BeanJamTranscribe</code>    - Runtime: Node.js 20.x    - Architecture: x86_64    - Execution role: Use existing role \u2192 <code>BeanJamTranscribeLambdaRole</code> 2. Upload <code>function.zip</code> (Code \u2192 Upload from \u2192 .zip file) 3. Configuration \u2192 Environment variables \u2192 Add:    - Key: <code>GOOGLE_SERVICE_ACCOUNT_KEY</code>    - Value: (paste entire Google service account JSON) 4. Configuration \u2192 General configuration:    - Timeout: 30 seconds (STT can take time)    - Memory: 512 MB</p> <p>Option B: AWS CLI </p><pre><code># Create Lambda function\naws lambda create-function \\\n  --function-name BeanJamTranscribe \\\n  --runtime nodejs20.x \\\n  --role arn:aws:iam::YOUR_ACCOUNT_ID:role/BeanJamTranscribeLambdaRole \\\n  --handler index.handler \\\n  --zip-file fileb://function.zip \\\n  --timeout 30 \\\n  --memory-size 512 \\\n  --environment Variables=\"{GOOGLE_SERVICE_ACCOUNT_KEY='PASTE_JSON_HERE'}\"\n</code></pre><p></p>"},{"location":"deployment/aws-lambda/#step-4-create-api-gateway-http-api","title":"Step 4: Create API Gateway (HTTP API)","text":"<p>AWS Console: 1. API Gateway \u2192 Create API \u2192 HTTP API (not REST) 2. Integrations: Add integration    - Integration type: Lambda    - Lambda function: <code>BeanJamTranscribe</code>    - Version: 2.0 3. Routes: Configure routes    - Method: POST    - Path: <code>/transcribe</code> 4. CORS: Configure CORS    - Allow origins: <code>*</code> (or your domain for production)    - Allow methods: POST, OPTIONS    - Allow headers: Content-Type 5. Deploy: Create stage    - Stage name: <code>prod</code>    - Copy the Invoke URL (e.g., <code>https://abc123.execute-api.us-east-1.amazonaws.com/prod</code>)</p> <p>Or via CLI: </p><pre><code># Create HTTP API\nAPI_ID=$(aws apigatewayv2 create-api \\\n  --name BeanJamTranscribeAPI \\\n  --protocol-type HTTP \\\n  --cors-configuration AllowOrigins='*',AllowMethods='POST,OPTIONS',AllowHeaders='Content-Type' \\\n  --query 'ApiId' --output text)\n\n# Create integration\nINTEGRATION_ID=$(aws apigatewayv2 create-integration \\\n  --api-id $API_ID \\\n  --integration-type AWS_PROXY \\\n  --integration-uri arn:aws:lambda:REGION:ACCOUNT_ID:function:BeanJamTranscribe \\\n  --payload-format-version 2.0 \\\n  --query 'IntegrationId' --output text)\n\n# Create route\naws apigatewayv2 create-route \\\n  --api-id $API_ID \\\n  --route-key 'POST /transcribe' \\\n  --target integrations/$INTEGRATION_ID\n\n# Create stage and deploy\naws apigatewayv2 create-stage \\\n  --api-id $API_ID \\\n  --stage-name prod \\\n  --auto-deploy\n\n# Get invoke URL\necho \"https://${API_ID}.execute-api.REGION.amazonaws.com/prod\"\n</code></pre><p></p>"},{"location":"deployment/aws-lambda/#step-5-grant-api-gateway-permission-to-invoke-lambda","title":"Step 5: Grant API Gateway Permission to Invoke Lambda","text":"<pre><code>aws lambda add-permission \\\n  --function-name BeanJamTranscribe \\\n  --statement-id apigateway-invoke \\\n  --action lambda:InvokeFunction \\\n  --principal apigatewayv2.amazonaws.com \\\n  --source-arn \"arn:aws:execute-api:REGION:ACCOUNT_ID:API_ID/*/*\"\n</code></pre>"},{"location":"deployment/aws-lambda/#step-6-update-frontend-configuration","title":"Step 6: Update Frontend Configuration","text":"<p>Add the API Gateway URL to your <code>.env</code>: </p><pre><code>VITE_TRANSCRIBE_API_URL=https://abc123.execute-api.us-east-1.amazonaws.com/prod/transcribe\n</code></pre><p></p> <p>Update <code>src/hooks/use-audio-recorder.ts</code> to use the new endpoint: </p><pre><code>async function sendToServer(blob: Blob, languageCode = 'en-US') {\n  const arrayBuffer = await blob.arrayBuffer();\n  const base64 = bufferToBase64(arrayBuffer);\n\n  // Use API Gateway endpoint if available, otherwise fallback to local\n  const endpoint = import.meta.env.VITE_TRANSCRIBE_API_URL || '/api/transcribe';\n\n  const resp = await fetch(endpoint, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ audioBase64: base64, mimeType: blob.type, languageCode }),\n  });\n  if (!resp.ok) throw new Error(await resp.text());\n  return resp.json();\n}\n</code></pre><p></p>"},{"location":"deployment/aws-lambda/#step-7-test-the-lambda-function","title":"Step 7: Test the Lambda Function","text":"<p>Test from AWS Console: 1. Lambda \u2192 Functions \u2192 BeanJamTranscribe \u2192 Test 2. Create test event with sample audio base64: </p><pre><code>{\n  \"httpMethod\": \"POST\",\n  \"body\": \"{\\\"audioBase64\\\":\\\"SAMPLE_BASE64\\\",\\\"mimeType\\\":\\\"audio/webm\\\",\\\"languageCode\\\":\\\"en-US\\\"}\"\n}\n</code></pre><p></p> <p>Test via curl: </p><pre><code>curl -X POST https://YOUR_API_GATEWAY_URL/transcribe \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"audioBase64\":\"YOUR_BASE64\",\"mimeType\":\"audio/webm\",\"languageCode\":\"en-US\"}'\n</code></pre><p></p>"},{"location":"deployment/aws-lambda/#cost-estimates-aws-free-tier","title":"Cost Estimates (AWS Free Tier)","text":"<ul> <li>Lambda: 1M requests/month free, 400,000 GB-seconds compute free</li> <li>API Gateway: 1M requests/month free (first 12 months)</li> <li>CloudWatch Logs: 5 GB ingestion, 5 GB archive free</li> </ul> <p>Typical usage: ~1000 transcriptions/month = $0-2/month (well within free tier).</p>"},{"location":"deployment/aws-lambda/#monitoring-troubleshooting","title":"Monitoring &amp; Troubleshooting","text":"<p>View logs: </p><pre><code>aws logs tail /aws/lambda/BeanJamTranscribe --follow\n</code></pre><p></p> <p>Or in Console: - Lambda \u2192 Functions \u2192 BeanJamTranscribe \u2192 Monitor \u2192 View CloudWatch logs</p> <p>Common errors: - Timeout: Increase Lambda timeout to 60s for large audio files - Memory: Increase to 1024 MB if processing large files - CORS: Ensure API Gateway CORS is configured for your domain - Credentials: Verify <code>GOOGLE_SERVICE_ACCOUNT_KEY</code> is valid JSON</p>"},{"location":"deployment/aws-lambda/#interview-talking-points","title":"Interview Talking Points \ud83c\udfa4","text":"<p>\"I migrated the transcription service to AWS Lambda with API Gateway to demonstrate serverless architecture: - Lambda function handles Google Cloud STT API calls - API Gateway HTTP API provides RESTful endpoint with CORS - IAM roles follow least-privilege principle - CloudWatch Logs for monitoring and debugging - Cost-effective: within AWS Free Tier (~$0-2/month) - Scalable: auto-scales to handle traffic spikes - Maintains compatibility with existing Vercel deployment via fallback\"</p>"},{"location":"deployment/production/","title":"Production","text":""},{"location":"deployment/production/#production-deployment-guide","title":"Production Deployment Guide","text":""},{"location":"deployment/production/#overview","title":"Overview","text":"<p>This app has two components that need to be deployed: 1. Frontend: React/Vite static site 2. Backend: Express server for Google Cloud Speech-to-Text</p>"},{"location":"deployment/production/#deployment-options","title":"Deployment Options","text":""},{"location":"deployment/production/#option-1-vercel-frontend-vercel-serverless-function-backend","title":"Option 1: Vercel (Frontend) + Vercel Serverless Function (Backend)","text":"<p>Best for: Simple, managed deployment</p>"},{"location":"deployment/production/#setup-steps","title":"Setup Steps:","text":"<ol> <li> <p>Install Vercel CLI </p><pre><code>npm install -g vercel\n</code></pre><p></p> </li> <li> <p>Create <code>api/transcribe.js</code> for Vercel Serverless (Already created - see file below)</p> </li> <li> <p>Update <code>vercel.json</code> configuration (Already created - see file below)</p> </li> <li> <p>Set Environment Variables in Vercel Dashboard</p> </li> <li>Go to your Vercel project \u2192 Settings \u2192 Environment Variables</li> <li> <p>Add: <code>GOOGLE_APPLICATION_CREDENTIALS_JSON</code> = (paste entire JSON content from your key file)</p> </li> <li> <p>Deploy </p><pre><code>vercel --prod\n</code></pre><p></p> </li> </ol>"},{"location":"deployment/production/#option-2-netlify-frontend-netlify-functions-backend","title":"Option 2: Netlify (Frontend) + Netlify Functions (Backend)","text":"<p>Best for: Similar to Vercel, alternative platform</p>"},{"location":"deployment/production/#setup-steps_1","title":"Setup Steps:","text":"<ol> <li> <p>Install Netlify CLI </p><pre><code>npm install -g netlify-cli\n</code></pre><p></p> </li> <li> <p>Create <code>netlify/functions/transcribe.js</code> (Already created - see file below)</p> </li> <li> <p>Update <code>netlify.toml</code> (Already created - see file below)</p> </li> <li> <p>Set Environment Variables </p><pre><code>netlify env:set GOOGLE_APPLICATION_CREDENTIALS_JSON \"paste-json-here\"\n</code></pre><p></p> </li> <li> <p>Deploy </p><pre><code>netlify deploy --prod\n</code></pre><p></p> </li> </ol>"},{"location":"deployment/production/#option-3-traditional-vpscloud-frontend-backend-separate","title":"Option 3: Traditional VPS/Cloud (Frontend + Backend Separate)","text":"<p>Best for: Full control, existing infrastructure</p>"},{"location":"deployment/production/#backend-express-server-on-cloud-vm","title":"Backend (Express Server on Cloud VM)","text":"<ol> <li>Deploy to VM (AWS EC2, Google Cloud VM, Azure VM, DigitalOcean, etc.)</li> </ol> <pre><code># On your server:\ngit clone &lt;your-repo&gt;\ncd Bean-Jam-Bot\nnpm install --production\n\n# Upload your service account JSON to secure location\n# Set environment variable\nexport GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json\n\n# Start with PM2 (process manager)\nnpm install -g pm2\npm2 start server/index.js --name bean-jam-server\npm2 save\npm2 startup\n</code></pre> <ol> <li>Configure Nginx reverse proxy <pre><code>server {\n    listen 80;\n    server_name api.yourdomain.com;\n\n    location /api {\n        proxy_pass http://localhost:3001;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection 'upgrade';\n        proxy_set_header Host $host;\n        proxy_cache_bypass $http_upgrade;\n    }\n}\n</code></pre></li> </ol>"},{"location":"deployment/production/#frontend-static-site","title":"Frontend (Static Site)","text":"<ol> <li> <p>Build </p><pre><code>npm run build\n</code></pre><p></p> </li> <li> <p>Deploy <code>dist/</code> folder to:</p> </li> <li>Vercel: <code>vercel --prod</code></li> <li>Netlify: <code>netlify deploy --prod --dir=dist</code></li> <li>AWS S3 + CloudFront: Upload <code>dist/</code> to S3 bucket, configure CloudFront</li> <li> <p>Traditional server: Copy <code>dist/</code> to <code>/var/www/html</code></p> </li> <li> <p>Update frontend to point to backend Create <code>.env.production</code>: </p><pre><code>VITE_API_BASE_URL=https://api.yourdomain.com\n</code></pre><p></p> </li> </ol> <p>Update <code>src/hooks/use-audio-recorder.ts</code>: </p><pre><code>const apiUrl = import.meta.env.VITE_API_BASE_URL || '';\nconst resp = await fetch(`${apiUrl}/api/transcribe`, { ... });\n</code></pre><p></p>"},{"location":"deployment/production/#option-4-docker-containerized","title":"Option 4: Docker (Containerized)","text":"<p>Best for: Kubernetes, container orchestration</p>"},{"location":"deployment/production/#dockerfile-backend","title":"Dockerfile (Backend)","text":"<p>(Already created - see file below)</p>"},{"location":"deployment/production/#deploy-to-cloud-run-google-cloud","title":"Deploy to Cloud Run (Google Cloud)","text":"<pre><code># Build and push\ngcloud builds submit --tag gcr.io/YOUR_PROJECT/bean-jam-server\n\n# Deploy\ngcloud run deploy bean-jam-server \\\n  --image gcr.io/YOUR_PROJECT/bean-jam-server \\\n  --platform managed \\\n  --region us-central1 \\\n  --allow-unauthenticated \\\n  --set-env-vars GOOGLE_APPLICATION_CREDENTIALS=/app/service-account.json\n</code></pre>"},{"location":"deployment/production/#security-checklist-for-production","title":"Security Checklist for Production","text":"<p>\u2705 Never commit service account JSON to git - Already in <code>.gitignore</code> - Use environment variables or secrets manager</p> <p>\u2705 Use HTTPS - Vercel/Netlify provide this automatically - For VPS: Use Let's Encrypt (certbot)</p> <p>\u2705 Set CORS properly Update <code>server/index.js</code>: </p><pre><code>app.use(cors({\n  origin: process.env.FRONTEND_URL || 'https://yourdomain.com',\n  credentials: true\n}));\n</code></pre><p></p> <p>\u2705 Rate limiting </p><pre><code>npm install express-rate-limit\n</code></pre><p></p> <pre><code>import rateLimit from 'express-rate-limit';\n\nconst limiter = rateLimit({\n  windowMs: 15 * 60 * 1000, // 15 minutes\n  max: 100 // limit each IP to 100 requests per windowMs\n});\n\napp.use('/api/', limiter);\n</code></pre> <p>\u2705 Authenticate users - Add API keys or OAuth - Validate requests before transcription</p> <p>\u2705 Monitor costs - Google Cloud STT charges per second - Set up billing alerts in Google Cloud Console</p>"},{"location":"deployment/production/#environment-variables-summary","title":"Environment Variables Summary","text":""},{"location":"deployment/production/#development-env","title":"Development (.env)","text":"<pre><code>GOOGLE_APPLICATION_CREDENTIALS=./src/Keys/service-account.json\nVITE_GEMINI_API_KEY=your_key\nVITE_WEATHER_API_KEY=your_key\n</code></pre>"},{"location":"deployment/production/#production-vercelnetlify","title":"Production (Vercel/Netlify)","text":"<pre><code>GOOGLE_APPLICATION_CREDENTIALS_JSON={\"type\":\"service_account\",...}\nVITE_GEMINI_API_KEY=your_key\nVITE_WEATHER_API_KEY=your_key\n</code></pre>"},{"location":"deployment/production/#production-vps","title":"Production (VPS)","text":"<pre><code>export GOOGLE_APPLICATION_CREDENTIALS=/secure/path/service-account.json\nexport PORT=3001\nexport NODE_ENV=production\n</code></pre>"},{"location":"deployment/production/#quick-deploy-commands","title":"Quick Deploy Commands","text":""},{"location":"deployment/production/#vercel-recommended-for-beginners","title":"Vercel (Recommended for beginners)","text":"<pre><code># First time\nvercel\n\n# Set env vars in dashboard: https://vercel.com/[your-project]/settings/environment-variables\n\n# Deploy\nvercel --prod\n</code></pre>"},{"location":"deployment/production/#netlify","title":"Netlify","text":"<pre><code>netlify init\nnetlify env:set GOOGLE_APPLICATION_CREDENTIALS_JSON \"paste-json-content\"\nnetlify deploy --prod\n</code></pre>"},{"location":"deployment/production/#docker-cloud-run","title":"Docker + Cloud Run","text":"<pre><code>gcloud run deploy bean-jam-api --source . --region us-central1\n</code></pre>"},{"location":"deployment/production/#troubleshooting-production-issues","title":"Troubleshooting Production Issues","text":""},{"location":"deployment/production/#could-not-load-credentials","title":"\"Could not load credentials\"","text":"<ul> <li>Ensure <code>GOOGLE_APPLICATION_CREDENTIALS_JSON</code> is set in platform dashboard</li> <li>For Vercel/Netlify functions, parse JSON from env var (see api/transcribe.js)</li> </ul>"},{"location":"deployment/production/#cors-errors","title":"CORS errors","text":"<ul> <li>Update <code>cors()</code> origin to match your frontend domain</li> <li>Check browser console for exact error</li> </ul>"},{"location":"deployment/production/#404-on-apitranscribe","title":"404 on /api/transcribe","text":"<ul> <li>Vercel: Ensure <code>api/transcribe.js</code> exists</li> <li>Netlify: Ensure <code>netlify/functions/transcribe.js</code> exists</li> <li>VPS: Check nginx proxy configuration</li> </ul>"},{"location":"deployment/production/#high-latency","title":"High latency","text":"<ul> <li>Use streaming recognition for real-time (not implemented in this basic version)</li> <li>Deploy backend in same region as frontend</li> <li>Consider CDN for frontend assets</li> </ul>"},{"location":"deployment/production/#cost-estimation","title":"Cost Estimation","text":"<p>Google Cloud Speech-to-Text Pricing: - Standard: $0.006 per 15 seconds ($0.024/minute) - Enhanced: $0.009 per 15 seconds ($0.036/minute)</p> <p>Example: 1000 users, 2 minutes audio/day = $48-72/month</p> <p>Free tier: 60 minutes/month</p> <p>Monitor usage: https://console.cloud.google.com/billing</p>"},{"location":"deployment/production/#next-steps","title":"Next Steps","text":"<ol> <li>Choose deployment option (I recommend Vercel for simplicity)</li> <li>Set up environment variables in platform dashboard</li> <li>Deploy with one command</li> <li>Test microphone functionality in production</li> <li>Set up monitoring and alerts</li> </ol> <p>Need help with a specific platform? Let me know!</p>"},{"location":"deployment/quick-deploy/","title":"Quick Deploy","text":""},{"location":"deployment/quick-deploy/#production-deployment-quick-start","title":"\ud83d\ude80 Production Deployment - Quick Start","text":""},{"location":"deployment/quick-deploy/#easiest-option-vercel-recommended","title":"Easiest Option: Vercel (Recommended)","text":""},{"location":"deployment/quick-deploy/#1-one-command-deploy","title":"1\ufe0f\u20e3 One-Command Deploy","text":"<pre><code># Windows PowerShell\n.\\deploy-prod.ps1\n</code></pre> <p>Or manually:</p> <pre><code># Install Vercel CLI\nnpm install -g vercel\n\n# Build and deploy\nnpm run build\nvercel --prod\n</code></pre>"},{"location":"deployment/quick-deploy/#2-set-environment-variables","title":"2\ufe0f\u20e3 Set Environment Variables","text":"<p>After first deploy, go to: https://vercel.com/[your-project]/settings/environment-variables</p> <p>Add these three variables:</p> Variable Name Value Where to get it <code>GOOGLE_APPLICATION_CREDENTIALS_JSON</code> Entire JSON content from <code>src/Keys/gen-lang-client-0056136731-d83579575479.json</code> Copy &amp; paste the full JSON <code>VITE_GEMINI_API_KEY</code> Your Gemini API key Already in your <code>.env</code> <code>VITE_WEATHER_API_KEY</code> Your Weather API key Already in your <code>.env</code> <p>Important: For <code>GOOGLE_APPLICATION_CREDENTIALS_JSON</code>: 1. Open your service account JSON file 2. Copy the entire contents (it should start with <code>{\"type\":\"service_account\",...}</code>) 3. Paste it as the value (Vercel will handle it correctly)</p>"},{"location":"deployment/quick-deploy/#3-redeploy","title":"3\ufe0f\u20e3 Redeploy","text":"<pre><code>vercel --prod\n</code></pre>"},{"location":"deployment/quick-deploy/#4-test","title":"4\ufe0f\u20e3 Test","text":"<p>Visit your production URL and click the microphone button!</p>"},{"location":"deployment/quick-deploy/#alternative-netlify","title":"Alternative: Netlify","text":"<pre><code># Install Netlify CLI\nnpm install -g netlify-cli\n\n# Login and initialize\nnetlify login\nnetlify init\n\n# Set environment variables\nnetlify env:set GOOGLE_APPLICATION_CREDENTIALS_JSON \"paste-json-here\"\nnetlify env:set VITE_GEMINI_API_KEY \"your-key\"\nnetlify env:set VITE_WEATHER_API_KEY \"your-key\"\n\n# Deploy\nnetlify deploy --prod\n</code></pre>"},{"location":"deployment/quick-deploy/#alternative-docker-cloud-run","title":"Alternative: Docker + Cloud Run","text":"<pre><code># Build Docker image\ndocker build -t bean-jam-server .\n\n# Test locally\ndocker run -p 3001:3001 \\\n  -e GOOGLE_APPLICATION_CREDENTIALS=/app/service-account.json \\\n  bean-jam-server\n\n# Deploy to Google Cloud Run\ngcloud builds submit --tag gcr.io/YOUR_PROJECT/bean-jam-server\ngcloud run deploy bean-jam-server \\\n  --image gcr.io/YOUR_PROJECT/bean-jam-server \\\n  --platform managed \\\n  --region us-central1 \\\n  --allow-unauthenticated\n</code></pre>"},{"location":"deployment/quick-deploy/#files-created-for-production","title":"Files Created for Production","text":"<p>\u2705 <code>api/transcribe.js</code> - Vercel serverless function \u2705 <code>netlify/functions/transcribe.js</code> - Netlify function \u2705 <code>vercel.json</code> - Vercel configuration \u2705 <code>netlify.toml</code> - Netlify configuration \u2705 <code>Dockerfile</code> - Docker container \u2705 <code>.dockerignore</code> - Docker ignore rules \u2705 <code>deploy-prod.ps1</code> - Quick deploy script \u2705 <code>PRODUCTION_DEPLOYMENT.md</code> - Full deployment guide</p>"},{"location":"deployment/quick-deploy/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/quick-deploy/#could-not-load-credentials","title":"\"Could not load credentials\"","text":"<ul> <li>Make sure you pasted the entire JSON content (not just the file path)</li> <li>Check that the JSON is valid (use a JSON validator)</li> </ul>"},{"location":"deployment/quick-deploy/#cors-errors","title":"CORS errors","text":"<ul> <li>Vercel and Netlify handle CORS automatically</li> <li>If using custom domain, update CORS settings in serverless function</li> </ul>"},{"location":"deployment/quick-deploy/#voice-recording-doesnt-work","title":"Voice recording doesn't work","text":"<ul> <li>Ensure HTTPS (Vercel/Netlify provide this automatically)</li> <li>Browser requires HTTPS for microphone access</li> <li>Check browser console for errors</li> </ul>"},{"location":"deployment/quick-deploy/#high-costs","title":"High costs","text":"<ul> <li>Monitor Google Cloud billing dashboard</li> <li>Each transcription costs ~$0.024 per minute</li> <li>Free tier: 60 minutes/month</li> </ul>"},{"location":"deployment/quick-deploy/#security-checklist","title":"Security Checklist","text":"<p>\u2705 Service account JSON is in <code>.gitignore</code> \u2705 Environment variables set in platform (not in code) \u2705 HTTPS enabled (automatic on Vercel/Netlify) \u2705 Consider adding rate limiting for production</p>"},{"location":"deployment/quick-deploy/#whats-next","title":"What's Next?","text":"<ul> <li>[ ] Set up custom domain</li> <li>[ ] Add authentication (protect API)</li> <li>[ ] Add rate limiting</li> <li>[ ] Monitor usage and costs</li> <li>[ ] Set up error tracking (Sentry)</li> <li>[ ] Add analytics</li> </ul>"},{"location":"deployment/quick-deploy/#cost-monitoring","title":"Cost Monitoring","text":"<p>Google Cloud Console: https://console.cloud.google.com/billing</p> <p>Set up billing alerts to avoid surprises!</p> <p>Estimated costs: - 100 users/day, 2 min each = ~$14/month - 1000 users/day, 2 min each = ~$140/month</p> <p>Need help? Check <code>PRODUCTION_DEPLOYMENT.md</code> for detailed guide.</p>"},{"location":"features/browser-compatibility/","title":"Browser Compatibility","text":""},{"location":"features/browser-compatibility/#browser-compatibility-guide","title":"Browser Compatibility Guide","text":""},{"location":"features/browser-compatibility/#supported-browsers","title":"Supported Browsers","text":"<p>Bean Jam Bot now works across all modern browsers with graceful degradation:</p>"},{"location":"features/browser-compatibility/#fully-supported","title":"\u2705 Fully Supported","text":"<ul> <li>Chrome/Chromium 47+ (Windows, Mac, Linux, Android)</li> <li>Microsoft Edge 12+ (Chromium-based)</li> <li>Firefox 36+ (Windows, Mac, Linux, Android)</li> <li>Safari 11+ (Mac, iOS)</li> <li>Opera 34+</li> </ul>"},{"location":"features/browser-compatibility/#limited-support","title":"\u26a0\ufe0f Limited Support","text":"<ul> <li>Older browsers: Text chat works, voice input may be unavailable</li> <li>Mobile browsers: Full support on iOS Safari 11+ and Chrome Android 47+</li> </ul>"},{"location":"features/browser-compatibility/#audio-format-support","title":"Audio Format Support","text":"<p>The app automatically detects and uses the best audio format for each browser:</p> Browser Primary Format Fallback Quality Chrome <code>audio/webm;codecs=opus</code> audio/wav Excellent Edge <code>audio/webm;codecs=opus</code> audio/wav Excellent Firefox <code>audio/ogg;codecs=opus</code> audio/wav Excellent Safari <code>audio/wav</code> - Good Opera <code>audio/webm;codecs=opus</code> audio/wav Excellent"},{"location":"features/browser-compatibility/#features-implemented","title":"Features Implemented","text":""},{"location":"features/browser-compatibility/#1-smart-format-detection","title":"1. Smart Format Detection","text":"<pre><code>// Automatically selects best format\nfunction getSupportedMimeType(): string {\n  const types = [\n    'audio/webm;codecs=opus',  // Chrome, Edge, Opera\n    'audio/ogg;codecs=opus',   // Firefox\n    'audio/wav',               // Safari, fallback\n  ];\n\n  for (const type of types) {\n    if (MediaRecorder.isTypeSupported(type)) {\n      return type;\n    }\n  }\n\n  return ''; // Browser default\n}\n</code></pre>"},{"location":"features/browser-compatibility/#2-enhanced-error-handling","title":"2. Enhanced Error Handling","text":"<ul> <li>Checks for <code>getUserMedia</code> support before recording</li> <li>Validates <code>MediaRecorder</code> availability</li> <li>Provides user-friendly error messages in EN/JP</li> <li>Logs detailed info to console for debugging</li> </ul>"},{"location":"features/browser-compatibility/#3-audio-quality-improvements","title":"3. Audio Quality Improvements","text":"<pre><code>audio: {\n  echoCancellation: true,   // Reduce echo\n  noiseSuppression: true,   // Filter background noise\n  autoGainControl: true,    // Normalize volume\n}\n</code></pre>"},{"location":"features/browser-compatibility/#4-browser-capability-detection","title":"4. Browser Capability Detection","text":"<p>Created <code>src/lib/browser-compat.ts</code> with utilities: - <code>checkBrowserCapabilities()</code> - Full feature detection - <code>isVoiceRecordingSupported()</code> - Quick check - <code>getCompatibilityMessage()</code> - User-friendly error messages - <code>logBrowserInfo()</code> - Console logging for debugging</p>"},{"location":"features/browser-compatibility/#5-backend-support","title":"5. Backend Support","text":"<p>Lambda transcription supports all formats: - \u2705 <code>WEBM_OPUS</code> (Chrome, Edge, Opera) - \u2705 <code>OGG_OPUS</code> (Firefox) - \u2705 <code>LINEAR16</code> (Safari WAV) - \u2705 <code>FLAC</code> (future compatibility)</p>"},{"location":"features/browser-compatibility/#user-experience","title":"User Experience","text":""},{"location":"features/browser-compatibility/#on-supported-browsers","title":"On Supported Browsers","text":"<ol> <li>Click microphone button</li> <li>Browser prompts for microphone permission</li> <li>Recording starts with visual feedback (blob animation + toast)</li> <li>Click again to stop</li> <li>Audio transcribes automatically</li> <li>Message sends to AI</li> </ol>"},{"location":"features/browser-compatibility/#on-unsupported-browsers","title":"On Unsupported Browsers","text":"<ol> <li>Click microphone button</li> <li>Friendly error message appears:</li> <li>EN: \"Voice recording requires microphone access. Please use a modern browser...\"</li> <li>JP: \"\u97f3\u58f0\u9332\u97f3\u975e\u5bfe\u5fdc...\"</li> <li>Text input still works perfectly</li> </ol>"},{"location":"features/browser-compatibility/#console-logging","title":"Console Logging","text":"<p>When the app loads, it logs browser info:</p> <pre><code>\ud83c\udf10 Browser Compatibility Check\nBrowser: Chrome 120\nMediaRecorder: \u2705\ngetUserMedia: \u2705\nAudioContext: \u2705\nGeolocation: \u2705\nSupported Audio Formats: ['audio/webm;codecs=opus', 'audio/webm', 'audio/wav']\n\u2705 All features supported!\n</code></pre> <p>During recording: </p><pre><code>Using audio format: audio/webm;codecs=opus\nMediaRecorder initialized with: audio/webm;codecs=opus\nSending transcription request: {\n  url: \"https://...\",\n  mimeType: \"audio/webm;codecs=opus\",\n  language: \"en-US\",\n  size: \"42.15 KB\"\n}\n</code></pre><p></p>"},{"location":"features/browser-compatibility/#testing","title":"Testing","text":""},{"location":"features/browser-compatibility/#manual-testing-checklist","title":"Manual Testing Checklist","text":"<ul> <li>[ ] Chrome: Record voice \u2192 transcribes \u2192 sends message</li> <li>[ ] Firefox: Record voice \u2192 transcribes \u2192 sends message</li> <li>[ ] Safari: Record voice \u2192 transcribes \u2192 sends message</li> <li>[ ] Edge: Record voice \u2192 transcribes \u2192 sends message</li> <li>[ ] Old browser: Shows error message, text input works</li> <li>[ ] Mobile Chrome: Voice recording works</li> <li>[ ] Mobile Safari: Voice recording works</li> </ul>"},{"location":"features/browser-compatibility/#cross-browser-test-urls","title":"Cross-Browser Test URLs","text":"<ul> <li>Chrome DevTools: Device Emulation</li> <li>Firefox: Responsive Design Mode</li> <li>Safari: Develop \u2192 User Agent \u2192 iPad/iPhone</li> <li>BrowserStack: Real device testing</li> </ul>"},{"location":"features/browser-compatibility/#troubleshooting","title":"Troubleshooting","text":""},{"location":"features/browser-compatibility/#error-getusermedia-not-supported","title":"Error: \"getUserMedia not supported\"","text":"<ul> <li>Cause: Very old browser or HTTP (not HTTPS)</li> <li>Solution: Update browser or use HTTPS</li> </ul>"},{"location":"features/browser-compatibility/#error-mediarecorder-not-supported","title":"Error: \"MediaRecorder not supported\"","text":"<ul> <li>Cause: Browser doesn't support MediaRecorder API</li> <li>Solution: Update to latest version</li> </ul>"},{"location":"features/browser-compatibility/#error-no-audio-formats-are-supported","title":"Error: \"No audio formats are supported\"","text":"<ul> <li>Cause: Severely outdated browser</li> <li>Solution: Upgrade browser or use text input</li> </ul>"},{"location":"features/browser-compatibility/#error-microphone-permission-denied","title":"Error: \"Microphone permission denied\"","text":"<ul> <li>Cause: User blocked microphone access</li> <li>Solution: </li> <li>Click lock icon in address bar</li> <li>Reset permissions</li> <li>Refresh page</li> </ul>"},{"location":"features/browser-compatibility/#safari-specific-issues","title":"Safari-Specific Issues","text":"<ul> <li>Issue: Large WAV files</li> <li>Impact: Slower upload times</li> <li>Mitigation: Backend handles all formats equally</li> </ul>"},{"location":"features/browser-compatibility/#future-enhancements","title":"Future Enhancements","text":""},{"location":"features/browser-compatibility/#potential-improvements","title":"Potential Improvements","text":"<ul> <li>[ ] Add file upload for pre-recorded audio</li> <li>[ ] Support for more audio formats (MP4, AAC)</li> <li>[ ] Audio compression before upload</li> <li>[ ] Progressive Web App (PWA) for offline capability</li> <li>[ ] WebRTC for real-time streaming transcription</li> </ul>"},{"location":"features/browser-compatibility/#browser-apis-to-watch","title":"Browser APIs to Watch","text":"<ul> <li>WebCodecs API: Better audio encoding control</li> <li>Media Capture: Enhanced audio constraints</li> <li>WebAssembly: Client-side audio processing</li> </ul>"},{"location":"features/browser-compatibility/#resources","title":"Resources","text":"<ul> <li>MDN: MediaRecorder API</li> <li>Can I Use: MediaRecorder</li> <li>Google Speech-to-Text: Audio Encoding</li> <li>Web Audio API</li> </ul>"},{"location":"features/browser-compatibility/#support-matrix","title":"Support Matrix","text":"Feature Chrome Firefox Safari Edge Opera Text Chat \u2705 \u2705 \u2705 \u2705 \u2705 Voice Input \u2705 \u2705 \u2705 \u2705 \u2705 Audio Visualization \u2705 \u2705 \u2705 \u2705 \u2705 Weather Cards \u2705 \u2705 \u2705 \u2705 \u2705 Geolocation \u2705 \u2705 \u2705 \u2705 \u2705 Blob Animation \u2705 \u2705 \u2705 \u2705 \u2705 <p>Last Updated: October 18, 2025 Tested Browsers: Chrome 120, Firefox 121, Safari 17, Edge 120, Opera 105</p>"},{"location":"features/weather-detection/","title":"Weather Detection","text":""},{"location":"features/weather-detection/#gemini-driven-weather-card-display","title":"Gemini-Driven Weather Card Display","text":""},{"location":"features/weather-detection/#overview","title":"Overview","text":"<p>The weather card display is now controlled by Gemini AI, allowing it to intelligently decide when weather information is relevant to show the user.</p>"},{"location":"features/weather-detection/#how-it-works","title":"How It Works","text":""},{"location":"features/weather-detection/#1-weather-data-collection","title":"1. Weather Data Collection","text":"<ul> <li>Weather data is always fetched when a user sends a message (if location is available)</li> <li>The app passes current weather information to Gemini as context</li> <li>Date parsing still extracts forecast days from user messages (today, tomorrow, etc.)</li> </ul>"},{"location":"features/weather-detection/#2-gemini-decision-making","title":"2. Gemini Decision Making","text":"<p>Gemini receives weather data in its system instruction: </p><pre><code>Current weather data: [detailed weather info]\nIMPORTANT: If the user asks about weather-related questions, \nyou MUST start your response with [SHOW_WEATHER_CARD] marker.\n</code></pre><p></p>"},{"location":"features/weather-detection/#3-response-parsing","title":"3. Response Parsing","text":"<p>When Gemini responds: - The app checks for the <code>[SHOW_WEATHER_CARD]</code> marker in the response - If found: Display the weather card + Gemini's commentary - If not found: Display only Gemini's response (no card)</p>"},{"location":"features/weather-detection/#4-smart-detection","title":"4. Smart Detection","text":"<p>Gemini can now detect weather-related questions more intelligently: - Direct: \"What's the weather today?\" - Indirect: \"Should I bring an umbrella?\" - Contextual: \"Is it a good day for a picnic?\" - Multi-language: \"\u4eca\u65e5\u306e\u5929\u6c17\u306f\u3069\u3046\u3067\u3059\u304b\uff1f\"</p>"},{"location":"features/weather-detection/#architecture","title":"Architecture","text":"<pre><code>User Message\n    \u2193\n[Always fetch weather data if location available]\n    \u2193\nPass weather + location to Gemini\n    \u2193\nGemini analyzes if weather card is relevant\n    \u2193\n    \u251c\u2500 Weather relevant \u2192 Response includes [SHOW_WEATHER_CARD]\n    \u2502                     \u2192 Display card + commentary\n    \u2502\n    \u2514\u2500 Not relevant \u2192 Normal response\n                     \u2192 Display only text\n</code></pre>"},{"location":"features/weather-detection/#benefits","title":"Benefits","text":""},{"location":"features/weather-detection/#1-smarter-detection","title":"1. Smarter Detection","text":"<ul> <li>Gemini understands context better than keyword matching</li> <li>Can detect indirect weather questions</li> <li>Works across different phrasings and languages</li> </ul>"},{"location":"features/weather-detection/#2-no-false-positives","title":"2. No False Positives","text":"<ul> <li>Keywords like \"cloud\" in \"cloud storage\" won't trigger weather card</li> <li>Gemini uses context to determine relevance</li> </ul>"},{"location":"features/weather-detection/#3-better-ux","title":"3. Better UX","text":"<ul> <li>Weather card only shows when actually needed</li> <li>Natural conversation flow</li> <li>AI decides best presentation</li> </ul>"},{"location":"features/weather-detection/#4-flexibility","title":"4. Flexibility","text":"<ul> <li>Gemini can show weather for:</li> <li>Direct weather questions</li> <li>Activity planning questions</li> <li>Travel advice</li> <li>Any context where weather matters</li> </ul>"},{"location":"features/weather-detection/#example-interactions","title":"Example Interactions","text":""},{"location":"features/weather-detection/#weather-relevant-card-shows","title":"Weather Relevant (Card Shows)","text":"<p>User: \"Should I bring a jacket today?\" Gemini Response: </p><pre><code>[SHOW_WEATHER_CARD]\nIt's 15\u00b0C and cloudy today, so I'd recommend bringing a light jacket. \nThere's a 30% chance of rain, so you might want to grab an umbrella too!\n</code></pre> Display: Weather card + commentary<p></p>"},{"location":"features/weather-detection/#not-weather-relevant-no-card","title":"Not Weather Relevant (No Card)","text":"<p>User: \"What's cloud computing?\" Gemini Response: </p><pre><code>Cloud computing refers to the delivery of computing services...\n</code></pre> Display: Text only (even though \"cloud\" mentioned)<p></p>"},{"location":"features/weather-detection/#technical-implementation","title":"Technical Implementation","text":""},{"location":"features/weather-detection/#modified-files","title":"Modified Files","text":"<ol> <li><code>src/lib/gemini.ts</code></li> <li>Changed return type to <code>{ response: string; showWeatherCard: boolean }</code></li> <li>Added <code>[SHOW_WEATHER_CARD]</code> marker detection</li> <li> <p>Updated system instruction with card display rules</p> </li> <li> <p><code>src/pages/Index.tsx</code></p> </li> <li>Removed keyword-based weather detection</li> <li>Always fetch weather data when location available</li> <li>Check <code>result.showWeatherCard</code> flag to determine card display</li> <li>Pass weatherCardData only when Gemini indicates</li> </ol>"},{"location":"features/weather-detection/#marker-protocol","title":"Marker Protocol","text":"<ul> <li>Marker: <code>[SHOW_WEATHER_CARD]</code></li> <li>Position: Anywhere in Gemini's response</li> <li>Processing: Removed from final display text</li> <li>Action: Triggers weather card rendering</li> </ul>"},{"location":"features/weather-detection/#future-enhancements","title":"Future Enhancements","text":"<ol> <li>Multiple Card Types</li> <li>Add markers for different data visualizations</li> <li> <p><code>[SHOW_MAP]</code>, <code>[SHOW_CHART]</code>, etc.</p> </li> <li> <p>Card Configuration</p> </li> <li>Let Gemini specify which weather details to emphasize</li> <li> <p><code>[SHOW_WEATHER_CARD:forecast=3days]</code></p> </li> <li> <p>Dynamic Updates</p> </li> <li>Real-time weather updates in conversation</li> <li> <p>Automatic refresh for long conversations</p> </li> <li> <p>Historical Context</p> </li> <li>Remember when weather was last shown</li> <li>Avoid redundant card displays</li> </ol>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#installation-guide","title":"Installation Guide","text":"<p>This guide will help you set up Bean Jam Bot for local development.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Node.js 18.x or higher</li> <li>npm or bun package manager</li> <li>Git for version control</li> <li>Google Cloud Account (for Speech-to-Text API)</li> <li>Google AI Studio Account (for Gemini API)</li> <li>WeatherAPI.com Account (for weather data)</li> </ul>"},{"location":"getting-started/installation/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone https://github.com/Defalt-here/Bean-Jam-Bot.git\ncd Bean-Jam-Bot\n</code></pre>"},{"location":"getting-started/installation/#step-2-install-dependencies","title":"Step 2: Install Dependencies","text":"<p>Using npm: </p><pre><code>npm install\n</code></pre><p></p> <p>Or using bun (faster): </p><pre><code>bun install\n</code></pre><p></p>"},{"location":"getting-started/installation/#step-3-configure-environment-variables","title":"Step 3: Configure Environment Variables","text":"<p>Create a <code>.env</code> file in the root directory:</p> <pre><code>cp .env.example .env\n</code></pre> <p>Edit <code>.env</code> and add your API keys:</p> <pre><code># Google Gemini API Key (Required)\nVITE_GEMINI_API_KEY=your_gemini_api_key_here\n\n# Weather API Key (Required)\nVITE_WEATHER_API_KEY=your_weatherapi_key_here\n\n# Google Cloud Speech-to-Text API Key (Required for voice features)\nVITE_GOOGLE_CLOUD_API_KEY=your_google_cloud_api_key_here\n\n# Backend URLs (for production)\nVITE_TRANSCRIBE_API_URL=your_lambda_function_url_here\nVITE_WEATHER_PROXY_URL=your_weather_proxy_url_here\n</code></pre>"},{"location":"getting-started/installation/#getting-api-keys","title":"Getting API Keys","text":""},{"location":"getting-started/installation/#1-google-gemini-api-key","title":"1. Google Gemini API Key","text":"<ol> <li>Go to Google AI Studio</li> <li>Click \"Get API Key\"</li> <li>Create a new API key</li> <li>Copy and paste into <code>.env</code></li> </ol>"},{"location":"getting-started/installation/#2-weatherapicom-api-key","title":"2. WeatherAPI.com API Key","text":"<ol> <li>Sign up at WeatherAPI.com</li> <li>Go to your dashboard</li> <li>Copy your API key</li> <li>Paste into <code>.env</code></li> </ol>"},{"location":"getting-started/installation/#3-google-cloud-speech-to-text-api-key","title":"3. Google Cloud Speech-to-Text API Key","text":"<ol> <li>Go to Google Cloud Console</li> <li>Create a new project or select existing</li> <li>Enable \"Cloud Speech-to-Text API\"</li> <li>Create credentials (API Key)</li> <li>Copy and paste into <code>.env</code></li> </ol>"},{"location":"getting-started/installation/#step-4-run-development-server","title":"Step 4: Run Development Server","text":"<p>Start the development server:</p> <pre><code>npm run dev\n</code></pre> <p>Or with bun: </p><pre><code>bun run dev\n</code></pre><p></p> <p>The application will be available at <code>http://localhost:5173</code></p>"},{"location":"getting-started/installation/#step-5-build-for-production","title":"Step 5: Build for Production","text":"<p>To create a production build:</p> <pre><code>npm run build\n</code></pre> <p>Or: </p><pre><code>bun run build\n</code></pre><p></p> <p>Preview the production build: </p><pre><code>npm run preview\n</code></pre><p></p>"},{"location":"getting-started/installation/#project-structure","title":"Project Structure","text":"<pre><code>Bean-Jam-Bot/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 components/      # React components\n\u2502   \u2502   \u251c\u2500\u2500 BlobAnimation.tsx\n\u2502   \u2502   \u251c\u2500\u2500 ChatMessage.tsx\n\u2502   \u2502   \u251c\u2500\u2500 LanguageToggle.tsx\n\u2502   \u2502   \u2514\u2500\u2500 WeatherCard.tsx\n\u2502   \u251c\u2500\u2500 hooks/          # Custom React hooks\n\u2502   \u2502   \u251c\u2500\u2500 use-audio-level.ts\n\u2502   \u2502   \u2514\u2500\u2500 use-audio-recorder.ts\n\u2502   \u251c\u2500\u2500 lib/            # Utility libraries\n\u2502   \u2502   \u251c\u2500\u2500 gemini.ts   # Gemini AI integration\n\u2502   \u2502   \u251c\u2500\u2500 location.ts # Location services\n\u2502   \u2502   \u2514\u2500\u2500 weather.ts  # Weather API integration\n\u2502   \u2514\u2500\u2500 pages/          # Page components\n\u2502       \u2514\u2500\u2500 Index.tsx   # Main chat interface\n\u251c\u2500\u2500 aws-lambda/         # AWS Lambda functions\n\u251c\u2500\u2500 public/             # Static assets\n\u251c\u2500\u2500 docs/              # Documentation\n\u2514\u2500\u2500 package.json\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#issue-api-key-not-working","title":"Issue: API Key Not Working","text":"<p>Solution:  - Double-check your API keys in <code>.env</code> - Ensure <code>.env</code> is in the root directory - Restart the development server after changing <code>.env</code></p>"},{"location":"getting-started/installation/#issue-voice-recording-not-working","title":"Issue: Voice Recording Not Working","text":"<p>Solution: - Check browser compatibility (Chrome/Edge recommended) - Ensure HTTPS or localhost (required for microphone access) - Grant microphone permissions when prompted</p>"},{"location":"getting-started/installation/#issue-location-not-detected","title":"Issue: Location Not Detected","text":"<p>Solution: - Grant location permissions in browser - Ensure HTTPS or localhost (required for geolocation) - Check that OpenStreetMap Nominatim is accessible</p>"},{"location":"getting-started/installation/#issue-build-errors","title":"Issue: Build Errors","text":"<p>Solution: - Clear node_modules: <code>rm -rf node_modules &amp;&amp; npm install</code> - Clear cache: <code>npm cache clean --force</code> - Check Node.js version: <code>node --version</code> (should be 18+)</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>System Design - Understand the architecture</li> <li>Frontend Details - Learn about frontend implementation</li> <li>Backend Details - Learn about backend services</li> <li>Deployment Guide - Deploy your application</li> </ul>"},{"location":"getting-started/installation/#development-tips","title":"Development Tips","text":"<ol> <li>Hot Module Replacement (HMR) is enabled - changes reflect instantly</li> <li>TypeScript strict mode is enabled - helps catch errors early</li> <li>ESLint is configured - run <code>npm run lint</code> to check code quality</li> <li>Prettier formatting - use VS Code extension for auto-formatting</li> </ol>"},{"location":"getting-started/installation/#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please see our contributing guidelines in the main README.</p>"},{"location":"getting-started/overview/","title":"Overview","text":""},{"location":"getting-started/overview/#overview","title":"Overview","text":""},{"location":"getting-started/overview/#what-is-bean-jam-bot","title":"What is Bean Jam Bot?","text":"<p>Bean Jam Bot is a modern bilingual AI-powered restaurant and dating assistant that helps users plan perfect dates, discover great restaurants, and create memorable experiences. Built with cutting-edge technologies, it combines the power of Google Gemini AI with voice recognition and dynamic UI elements.</p>"},{"location":"getting-started/overview/#key-features","title":"Key Features","text":""},{"location":"getting-started/overview/#ai-powered-intelligence","title":"\ud83e\udd16 AI-Powered Intelligence","text":"<ul> <li>Google Gemini Pro integration for context-aware recommendations</li> <li>Natural language understanding for restaurant and dating planning</li> <li>Multi-turn conversation support with history tracking</li> <li>Smart itinerary planning assistance</li> </ul>"},{"location":"getting-started/overview/#voice-input","title":"\ud83c\udfa4 Voice Input","text":"<ul> <li>Google Cloud Speech-to-Text with 48kHz WEBM_OPUS encoding</li> <li>Bilingual support (English &amp; Japanese)</li> <li>Real-time audio visualization with 8-band equalizer</li> <li>Visual feedback with reactive blob animation</li> </ul>"},{"location":"getting-started/overview/#location-weather-aware","title":"\ud83c\udf0d Location &amp; Weather Aware","text":"<ul> <li>GPS and IP-based location detection</li> <li>Real-time weather integration</li> <li>Context-aware recommendations (indoor/outdoor based on weather)</li> <li>Supports both city names and GPS coordinates</li> </ul>"},{"location":"getting-started/overview/#modern-uiux","title":"\ud83c\udfa8 Modern UI/UX","text":"<ul> <li>Responsive design with Tailwind CSS</li> <li>Dynamic blob animation that reacts to voice input</li> <li>Smooth transitions and animations</li> <li>Dark/light mode support</li> </ul>"},{"location":"getting-started/overview/#bilingual-support","title":"\ud83c\udf10 Bilingual Support","text":"<ul> <li>Seamless language switching (EN \u2194 JP)</li> <li>Culturally relevant recommendations</li> <li>Localized UI strings and error messages</li> <li>Language-specific AI prompts</li> </ul>"},{"location":"getting-started/overview/#technology-stack","title":"Technology Stack","text":""},{"location":"getting-started/overview/#frontend","title":"Frontend","text":"<ul> <li>React 18 with TypeScript</li> <li>Vite for blazing-fast builds</li> <li>Tailwind CSS for styling</li> <li>Framer Motion for animations</li> </ul>"},{"location":"getting-started/overview/#backend","title":"Backend","text":"<ul> <li>AWS Lambda (Node.js 20.x) for serverless functions</li> <li>Google Cloud Speech-to-Text for transcription</li> <li>Google Gemini Pro for AI responses</li> <li>WeatherAPI.com for weather data</li> </ul>"},{"location":"getting-started/overview/#apis-used","title":"APIs Used","text":"<ul> <li>Google Gemini Pro API</li> <li>Google Cloud Speech-to-Text API</li> <li>WeatherAPI.com</li> <li>OpenStreetMap Nominatim (geocoding)</li> <li>ipapi.co (IP-based location)</li> </ul>"},{"location":"getting-started/overview/#use-cases","title":"Use Cases","text":"<ol> <li>Date Planning - Get personalized date ideas based on preferences, weather, and location</li> <li>Restaurant Discovery - Find the perfect restaurant for any occasion</li> <li>Restaurant Hopping - Plan multi-restaurant routes with cuisine diversity</li> <li>Quick Recommendations - Voice-activated instant suggestions</li> <li>Bilingual Support - Communicate in English or Japanese seamlessly</li> </ol>"},{"location":"getting-started/overview/#architecture","title":"Architecture","text":"<p>Bean Jam Bot follows a modern serverless architecture:</p> <pre><code>User Interface (React + Vite)\n        \u2193\nAWS Lambda Functions\n        \u2193\nExternal APIs (Gemini, Speech-to-Text, Weather)\n</code></pre> <p>For detailed architecture information, see the System Design page.</p>"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide - Set up your development environment</li> <li>System Design - Understand the architecture</li> <li>Deployment - Deploy your own instance</li> </ul>"}]}